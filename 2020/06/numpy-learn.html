
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

  <link
    href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/stylesheet/style.min.css">

  <link id="dark-theme-style" rel="stylesheet" type="text/css"   media="(prefers-color-scheme: dark)"      href="https://iliailmer.github.io/theme/stylesheet/dark-theme.min.css">

  <link id="pygments-dark-theme" rel="stylesheet" type="text/css"      media="(prefers-color-scheme: dark)"      href="https://iliailmer.github.io/theme/pygments/monokai.min.css">
  <link id="pygments-light-theme" rel="stylesheet" type="text/css"     media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"      href="https://iliailmer.github.io/theme/pygments/emacs.min.css">

  <link rel="stylesheet" href="https://iliailmer.github.io/theme/tipuesearch/tipuesearch.min.css" />

  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/solid.css">


  <link href="https://iliailmer.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
    title="Ilia Ilmer Atom">


  <link rel="shortcut icon" href="https://iliailmer.github.io/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="https://iliailmer.github.io/images/favicon.ico" type="image/x-icon">

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-131502498-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#333">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#333">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Microsoft EDGE -->
  <meta name="msapplication-TileColor" content="#333">


<meta name="author" content="Ilia Ilmer" />
<meta name="description" content="In this post, I expand on a little class/self-teaching project that I did during the Spring 2020 semester. NumPy-Learn: A Homemade Machine Learning Library Organization In this section we will discuss the main organization of the library: How the layers are built How loss functions work How a stochastic …" />
<meta name="keywords" content="machine learning, python, numpy, deep learning">


  <meta property="og:site_name" content="Ilia Ilmer"/>
  <meta property="og:title" content="NumPy-Learn, A Homemade Machine Learning Library"/>
  <meta property="og:description" content="In this post, I expand on a little class/self-teaching project that I did during the Spring 2020 semester. NumPy-Learn: A Homemade Machine Learning Library Organization In this section we will discuss the main organization of the library: How the layers are built How loss functions work How a stochastic …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://iliailmer.github.io/2020/06/numpy-learn.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2020-06-14 00:00:00-04:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://iliailmer.github.io/author/ilia-ilmer.html">
  <meta property="article:section" content="Posts"/>
  <meta property="article:tag" content="machine learning"/>
  <meta property="article:tag" content="python"/>
  <meta property="article:tag" content="numpy"/>
  <meta property="article:tag" content="deep learning"/>
  <meta property="og:image" content="https://iliailmer.github.io/images/compressed.jpeg">

  <title>Ilia Ilmer &ndash; NumPy-Learn, A Homemade Machine Learning Library</title>

</head>

<body >
  <aside>
    <div>
      <a href="https://iliailmer.github.io/">
        <img src="https://iliailmer.github.io/images/compressed.jpeg" alt="Ilia Ilmer" title="Ilia Ilmer">
      </a>

      <h1>
        <a href="https://iliailmer.github.io/">Ilia Ilmer</a>
      </h1>

<p>Algorithms and Coffee</p>
      <form class="navbar-search" action="https://iliailmer.github.io/search.html" role="search">
        <input type="text" name="q" id="tipue_search_input" placeholder="Search...">
      </form>

      <nav>
        <ul class="list">


          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/about.html#about">
              About
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/publications.html#publications">
              Publications
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/software.html#software">
              Software
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/talks.html#talks">
              Talks
            </a>
          </li>

        </ul>
      </nav>

      <ul class="social">
        <li>
          <a  class="sc-github" href="https://github.com/iliailmer" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        </li>
        <li>
          <a  class="sc-gitlab" href="https://gitlab.com/iliailmer" target="_blank">
            <i class="fab fa-gitlab"></i>
          </a>
        </li>
        <li>
          <a  class="sc-linkedin" href="https://linkedin.com/in/iilmer" target="_blank">
            <i class="fab fa-linkedin"></i>
          </a>
        </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="https://iliailmer.github.io/">Home</a>

      <a href="/files/resume.pdf">CV</a>

      <a href="https://iliailmer.github.io/feeds/all.atom.xml">Atom</a>

    </nav>

<article class="single">
  <header>
    
    <h1 id="numpy-learn">NumPy-Learn, A Homemade Machine Learning Library</h1>
    <p>
      Posted on Sun 14 June 2020 in <a href="https://iliailmer.github.io/category/posts.html">Posts</a>

    </p>
  </header>


  <nav class="toc">
    true
  </nav>
  <div>
    <p>In this post, I expand on a little class/self-teaching project that I did during the Spring 2020 semester.</p>
<h1 id="numpy-learn-a-homemade-machine-learning-library">NumPy-Learn: A Homemade Machine Learning Library</h1>
<h2 id="organization">Organization</h2>
<p>In this section we will discuss the main organization of the library:</p>
<ul>
<li>How the layers are built</li>
<li>How loss functions work</li>
<li>How a stochastic gradient descent optimizer was built</li>
</ul>
<p>After that, we introduce the class for building the neural net itself and explain how everything ties together. We conclude by performance analysis on a simple MNIST program.</p>
<p>Let us agree on convention similar to that of PyTorch library: we will call the main datatype <code>Tensor</code> instead of <code>array</code>, as follows:
<code>py
from numpy import ndarray as Tensor</code></p>
<p>This is to adhere to accepted aesthetics of most modern neural network libraries and nothing more. All methods are purely using <code>numpy</code> or <code>scipy</code>.</p>
<h3 id="linear-layer">Linear Layer</h3>
<p>Inspired by PyTorch, the naming convention here is the preserved. The design of the layer is also similar to PyTorch: the class <code>Linear</code> will have a <code>forward</code> and a <code>backward</code> methods. The former will represent the forward pass, that is, the passing of the input data through the layer towards the next. The latter is responsible for backward propagation of the gradient.</p>
<h4 id="forward-pass">Forward Pass</h4>
<p>Linear layer essentially represents matrix multiplication of the input data <span class="math">\(x\)</span> by a weight matrix <span class="math">\(W\)</span> with addition of bias <span class="math">\(b\)</span>:</p>
<div class="math">$$\mathrm{L}(x) = xW + b.$$</div>
<p>We require that the size of the input data was of the format <span class="math">\(batch\times input~features\)</span>, for instance if the input data has 784 pixel values, for a batch of 100 images the size of <span class="math">\(x\)</span> would be <span class="math">\(100\times 784\)</span> and the size of <span class="math">\(W\)</span> would be <span class="math">\(784\times out~features\)</span>, while <span class="math">\(b\)</span> is of the shape <span class="math">\(out~features\times 1\)</span>. Here we rely on <code>numpy</code> broadcasting the value of bias onto the resulting matrix <span class="math">\(xW\)</span> when adding <span class="math">\(b\)</span>.</p>
<p>In code, we define it as follows:</p>
<p><code>py
def forward(self, x: Tensor) -&gt; Tensor:
        self.input = x
        return x @ self.W + self.b</code></p>
<h4 id="gradient">Gradient</h4>
<p>In the forward pass, we computed the matrix product. Next, we need to evaluate the rate of change of the output of the current layer with respect to the input. Note, that due to the chain rule, the gradient flows from right (output) to left (input) as a product.</p>
<p>The <code>backward</code> method utilizes chainrule. It accepts the gradient from the layer <span class="math">\(l+1\)</span>, uses it to find the derivatives of the current layer's output with respect to <span class="math">\(W\)</span> and <span class="math">\(b\)</span> and finally, passes it along multiplying by the derivative of its output w.r.t. <span class="math">\(x\)</span>, the input.</p>
<p>Mathematically, this is the following:</p>
<ul>
<li>The loss's derivatives w.r.t. weights are <div class="math">$$\frac{\partial E}{\partial W^{l}_{ij}}=\sum\limits_{\text{input-output pair}}\delta^l_i out^{l-1}_j$$</div>.</li>
<li>Here, <span class="math">\(\delta^l_i\)</span> is the error of the <span class="math">\(l\)</span>th layer for <span class="math">\(i\)</span>th node: <div class="math">$$\delta^l_i=g'_{out}(a_i^l)\sum\limits_k W^{l+1}_{ik}\delta^{l+1}_{k}$$</div>, where <span class="math">\(g\)</span> is the activation function.</li>
</ul>
<p>These equations are written in a different shape convention, but we can take care of that in the code.</p>
<p>The derivative of <span class="math">\(xW+b\)</span> w.r.t. <span class="math">\(W\)</span> is <span class="math">\(x^T\)</span>. The derivative of <span class="math">\(xW+b\)</span> w.r.t. to <span class="math">\(b\)</span> is an identity matrix. Therefore, let <code>grad</code> be the gradient (error) received from <span class="math">\((l+1)\)</span>th layer, then we can define the <code>backward</code> method as below:
<code>py
def backward(self, grad: Tensor) -&gt; Tensor:
        # input_feat by batch_size @ batch_size by out_features
        self.dydw = self.input.T @ grad
        # we sum across batches and get shape (out_features)
        self.dydb = grad.sum(axis=0)
        # output must be of shape (batch_size, out_features)
        return grad @ self.W.T</code></p>
<p>Now we are ready to present the fully defined Linear Layer code below:</p>
<p>```python
import numpy as np
from numpy import ndarray as Tensor</p>
<p>class Linear:
    """A linear layer."""</p>
<pre><code>def __init__(self, in_features: int, out_features: int):
    """Initialize a linear layer with weights and biases."""
    self.W = np.random.randn(in_features, out_features)

    self.b = np.random.randn(out_features)

def forward(self, x: Tensor) -&gt; Tensor:
    """Compute forward pass, return W @ x + b.

    Arguments:
        W: the weight Tensor of shape (in_featuers, out_features)
        b: the bias vector of shape (out_features,)
        x: the input of shape (batch_size, in_features)

    Returns:
        A tensor of shape (batch_size, out_features)

    """
    self.input = x
    return x @ self.W + self.b

def backward(self, grad: Tensor) -&gt; Tensor:
    """Propagate the gradient from the l+1 layer to l-1 layer.

    Arguments:
        grad: the tensor gradients from the l+1 layer to be
              propagated, shape: (batch_size, out_features).

    References:
        http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html

    """
    # in_feat by batch_size @ batch_size by out_feat
    self.dydw = self.input.T @ grad
    # we sum across batches and get shape (out_features)
    self.dydb = grad.sum(axis=0)
    # output must be of shape (batch_size, out_features)
    return grad @ self.W.T

def __call__(self, x: Tensor) -&gt; Tensor:
    """Peform forward pass on `__call__`."""
    return self.forward(x)

def __repr__(self) -&gt; str:
    """Print a representation for Jupyter/IPython."""
    return f"""Linear Layer:\n\tWeight: {self.W.shape}"""\
        + f"""\n\tBias: {self.b.shape}"""
</code></pre>
<p>```</p>
<h3 id="layers-with-activation-functions">Layers with Activation Functions</h3>
<p>We define separate layers for activation functions, similarly to the way PyTorch handles those. We only define two here: ReLU and Sigmoid.</p>
<p>ReLU is defined as </p>
<div class="math">$$f(x) = \max\{0, x\}$$</div>
<p> and Sigmoid is defined as </p>
<div class="math">$$\sigma(x) = \frac{1}{1+\exp(-x)}.$$</div>
<p>Their derivatives are defined as </p>
<div class="math">$$(\nabla f)(t) = 0 \text{ if } t=0\text{, else }t$$</div>
<div class="math">$$(\nabla \sigma)(t) = \sigma(t)(1-\sigma(t))$$</div>
<p>The respective classes are defined below</p>
<p>```python
def sigmoid(x: Tensor) -&gt; Tensor:
    """Calculate the sigmoid function of x."""
    return 1/(1+np.exp(-x))</p>
<p>def sigmoid_prime(x: Tensor) -&gt; Tensor:
    """Calculate the d/dx of sigmoid function of x."""
    return sigmoid(x)*(1-sigmoid(x))</p>
<p>class ReLU:
    """ReLU class."""</p>
<pre><code>def __init__(self):
    """Initialize the ReLU instance."""

def forward(self, x: Tensor) -&gt; Tensor:
    """Compute the activation in the forward pass.

    Arguments:
        x: Tensor of inputs, shape (batch_size, in_features)

    Returns:
        Tensor of shape (batch_size, in_features)

    """
    return np.maximum(x, 0)

def backward(self, grad: Tensor) -&gt; Tensor:
    """Compute the gradient and pass it backwards.

    Arguments:
        grad: Tensor of gradients of shape (batch_size, out_features)

    Returns:
        Tensor of shape (batch_size, out_features)

    """
    return np.maximum(grad, 0)

def __call__(self, x: Tensor) -&gt; Tensor:
    """Peform forward pass on `__call__`."""
    return self.forward(x)

def __repr__(self) -&gt; str:
    """Print a representation of ReLU for Jupyter/IPython."""
    return """ReLU()"""
</code></pre>
<p>class Sigmoid:
    """Sigmoid class."""</p>
<pre><code>def __init__(self):
    """Initialize the instance.

    We add the main function for activation and its derivative function.
    """
    self.sigmoid = sigmoid
    self.sigmoid_prime = sigmoid_prime

def forward(self, x: Tensor) -&gt; Tensor:
    """Compute the activation in the forward pass.

    Arguments:
        x: Tensor of inputs with shape(batch_size, in_features)

    Returns:
        Tensor of shape(batch_size, in_features)

    """
    self.input = x
    return self.sigmoid(x)

def backward(self, grad: Tensor):
    """Compute the gradient and pass it backwards.

    Arguments:
        grad: Tensor of gradients with shape(batch_size, out_features)

    Returns:
        Tensor of shape(in_features, out_features)

    """
    return self.sigmoid_prime(self.input) * grad

def __call__(self, x: Tensor) -&gt; Tensor:
    """Peform forward pass on `__call__`."""
    return self.forward(x)

def __repr__(self) -&gt; str:
    """Print a representation of Sigmoid for Jupyter/IPython."""
    return """Sigmoid()"""
</code></pre>
<p>```</p>
<p>In addition to Sigmoid and ReLU, we also import <code>softmax</code> activation function from <code>scipy</code>. In my experiments, I found that this is the most stable implementation, so I did not want to run into <a href="https://en.wikipedia.org/wiki/Not_invented_here">"not invented here"</a> problem.</p>
<p><code>softmax</code> is defined as follows:</p>
<div class="math">$$\mathcal{S}(x)=\left[\frac{\exp(x_i)}{\sum\limits_{k}\exp(x_k)}\right], i=1..n,~x=[x_1, ... , x_n]$$</div>
<p>Softmax accepts a vector of network's output and converts it to a vector of probability values. For this function we need to use one-hot encoding.</p>
<p><code>python
from scipy.special import softmax as s
def softmax(x: Tensor) -&gt; Tensor:
    """Calculate softmax using scipy."""
    return s(x, axis=1)</code></p>
<h3 id="loss-functions">Loss Functions</h3>
<p>We implement two loss functions here. We will implement Mean Squared Error Loss class and a Cross Entropy Loss class.</p>
<h4 id="mean-squared-error-loss">Mean Squared Error Loss</h4>
<p>This is a very straight-forward loss function, it takes the output of the last layer of the neural network <span class="math">\(\hat{y}\)</span> and ccomputes:</p>
<div class="math">$$\mathcal{L}(y, \hat{y}) = \frac{1}{2m}||y-\hat{y}||^2,$$</div>
<p>where <span class="math">\(m\)</span> is the size of <span class="math">\(y\)</span> and </p>
<div class="math">$$\hat{y}$$</div>
<p> and </p>
<div class="math">$$\vert\vert...\vert\vert$$</div>
<p> represents the vector norm (sum of squared component-wise differences). The gradient of this function for backpropagation is computed as</p>
<div class="math">$$\nabla{\mathcal{L}}=\frac{1}{m}(y-\hat{y})$$</div>
<h4 id="cross-entropy-loss">Cross Entropy Loss</h4>
<p>Cross entropy loss function is defined as follows. Let <span class="math">\(\hat{y}\)</span> be the so-called <span class="math">\({logits}\)</span>, the outputs of the neural network. Then, we use softmax to calculate the probabilities <span class="math">\(p=\mathcal{S}\left(\hat{y}\right)\)</span>. The cross entropy is</p>
<div class="math">$$\mathcal{L}(y, \hat{y}) = -\sum\limits_{i} y_i \log p_i,$$</div>
<p>
where <span class="math">\(y_i\)</span> is the true label vector.</p>
<p>To evaluate the gradient, consider the following argument</p>
<div class="math">$$\nabla{\mathcal{L}}(y, \hat{y}) = -\sum\limits_i \frac{\partial \left(y_i \log [\mathcal{S}(x)]_i\right)}{\partial x_j}$$</div>
<p>
where <span class="math">\(x\)</span> is the network's output. Continuing this, we obtain</p>
<div class="math">$$\nabla{\mathcal{L}}(y, \hat{y}) = -\sum\limits_i y_i \frac{1}{p_i}\frac{\partial  \mathcal{S}(x)_i}{\partial x_j}$$</div>
<p>To find the derivative of softmax, consider</p>
<div class="math">$$\frac{\partial \mathcal{S}_i}{\partial x_j} = \frac{\partial}{\partial x_j}\left(\frac{\exp(x_i)}{\sum\limits_{k}\exp(x_k)}\right) = \frac{\frac{\partial\exp(x_i)}{\partial x_j} \sum\limits_{k}\exp(x_k) - \exp(x_i) \sum\limits_{k}\frac{\partial\exp(x_k)}{\partial x_j}}{\left(\sum\limits_{k}\exp(x_k)\right)^2},$$</div>
<div class="math">$$\frac{\partial \mathcal{S}_i}{\partial x_j} = \frac{\exp(x_i)\delta_{ij} \sum\limits_{k}\exp(x_k) - \exp(x_i)\exp(x_j)}{\left(\sum\limits_{k}\exp(x_k)\right)^2},$$</div>
<div class="math">$$\frac{\partial \mathcal{S}_i}{\partial x_j} = \frac{\exp(x_i)}{ \sum\limits_{k}\exp(x_k)}\delta_{ij} - \frac{\exp(x_i)}{\sum\limits_{k}\exp(x_k)}\frac{\exp(x_j)}{\sum\limits_{k}\exp(x_k)} = \mathcal{S}_i\delta_{ij} - \mathcal{S}_i \mathcal{S}_j.$$</div>
<p>We use <span class="math">\(\delta_{ij}\)</span> to represent Kronecker delta-symbol (essentially, identity matrix). Finally, we can interchange the notation <span class="math">\(\mathcal{S}_i\)</span> for <span class="math">\(p_i\)</span>, since both represent the <span class="math">\(i\)</span>th component of the softmax output (the probability)</p>
<div class="math">$$\nabla{\mathcal{L}}(y, \hat{y}) = -\sum\limits_i y_i \frac{1}{p_i}p_i(\delta_{ij}-p_j) = - y_j  + p_j\sum\limits_i y_i.$$</div>
<p>Recall, that the vector <span class="math">\(y\)</span> is one-hot encoded, therefore, the sum of its components <span class="math">\(\sum\limits_i y_i=1\)</span>. Hence, we obtain</p>
<div class="math">$$\nabla{\mathcal{L}}(y, \hat{y}) =p_j - y_j.$$</div>
<p>```python
class Loss:
    """Placeholder class for losses."""</p>
<pre><code>def __init__(self):
    """Initialize the class with 0 gradient."""
    self.grad = 0.

def grad_fn(self, pred: Tensor, true: Tensor) -&gt; Tensor:
    """Create placeholder for the gradient funtion."""
    pass

def loss_fn(self, pred: Tensor, true: Tensor) -&gt; Tensor:
    """Create placeholder for the loss funtion."""
    pass

def __call__(self, pred: Tensor, true: Tensor):
    """Calculate gradient and loss on call."""
    self.grad = self.grad_fn(pred, true)
    return self.loss_fn(pred, true)
</code></pre>
<p>class MSE(Loss):
    """Mean squared error loss."""</p>
<pre><code>def __init__(self):
    """Initialize via superclass."""
    super().__init__()

def grad_fn(self, pred: Tensor, true: Tensor) -&gt; Tensor:
    """Calculate the gradient of MSE.

    Args:
        pred: Tensor of predictions (raw output),
        shape (batch, )
        true: Tensor of true labels,
        shape (batch, )

    """
    return (pred - true)/true.shape[0]

def loss_fn(self, pred: Tensor, true: Tensor) -&gt; Tensor:
    """Calculate the MSE.

    Args:
        pred: Tensor of predictions (raw output),
        shape (batch,)
        true: Tensor of true labels (raw output),
        shape (batch,)

    """
    return 0.5*np.sum((pred - true)**2)/true.shape[0]

def __repr__(self):
    """Put pretty representation in Jupyter/IPython."""
    return """Mean Squared Error loss (pred: Tensor, true: Tensor)"""
</code></pre>
<p>class CrossEntropyLoss(Loss):
    """CrossEntropyLoss class."""</p>
<pre><code>def __init__(self) -&gt; None:
    """Initialize via superclass."""
    super().__init__()

def loss_fn(self, logits: Tensor, true: Tensor) -&gt; Tensor:
    """Calculate loss.

    Args:
        logits: Tensor of shape (batch size, number of classes),
        raw output of a neural network

        true: Tensor of shape (batch size,),
        a one-hot encoded vector

    """
    p = softmax(logits)
    return -np.mean(true * np.log(p))

def grad_fn(self, logits: Tensor, true: Tensor) -&gt; Tensor:
    """Calculate the gradient.

    Args:
        logits: Tensor of shape (batch size, number of classes),
        raw output of a neural network

        true: Tensor of shape (batch size, number of classes),
        a one-hot encoded vector

    """
    self.probabilities = softmax(logits)
    return self.probabilities - true
</code></pre>
<p>```</p>
<h2 id="building-the-network">Building the Network</h2>
<p>Here we describe the main class for our neural network. The main principle is simple, we pass a list of layers and initialize a class <code>Network</code> with two methods: <code>forward</code> and <code>backward</code>. The <code>forward</code> method performs the forward pass, that is, sends the input data through each layer. The <code>backward</code> method calls <code>backward</code> from each layer in the opposite direction (starting with the last layer). It uses the gradient of the lost function as its input.</p>
<p>```python
from typing import List, Union</p>
<p>Layer = Union[Linear, ReLU, Sigmoid]</p>
<p>class Network:
    """Basic Neural Network Class."""</p>
<pre><code>def __init__(self, layers: List[Layer]):
    """Initialize the Netowrk with a list of layers."""
    self.layers = layers[:]

def forward(self, x: Tensor):
    """Run the forward pass."""
    for l in self.layers:
        x = l(x)
    return x

def backward(self, grad: Tensor):
    """Run the backward pass."""
    for l in self.layers[::-1]:
        grad = l.backward(grad)
    return grad

def __call__(self, x: Tensor):
    """Run the forward pass on __call__."""
    return self.forward(x)

def __repr__(self) -&gt; str:
    """Print the representation for the network."""
    return "\n".join(l.__repr__() for l in self.layers)
</code></pre>
<p>```</p>
<h3 id="optimizers">Optimizers</h3>
<p>Our main optimizer here is going to be Stochastic Gradient Descent. After we computed the backpropagation, for every layer in the network, we are going to update the weights. If the gradient is <span class="math">\(\Delta w\)</span> then the update rule is </p>
<div class="math">$$w = w - \eta \Delta w - 2*\alpha w,$$</div>
<p>where <span class="math">\(\eta\)</span> is the learning rate and <span class="math">\(\alpha\)</span> is the <span class="math">\(L^2\)</span> regularization parameter. The code is presented below.</p>
<p>```python
class SGD:
    """Stochastic Gradient Descent class."""</p>
<pre><code>def __init__(self, lr: float, l2: float = 0.):
    """Initialize with learning rate and l2-regularization parameter."""
    self.lr = lr
    self.l2 = l2

def step(self, net: Network):
    """Perform optimization step."""
    for l in net.layers:
        if hasattr(l, 'dydw'):
            l.W = l.W - self.lr*l.dydw - 2 * self.l2 * l.W
        if hasattr(l, 'dydb'):
            l.b = l.b - self.lr*l.dydb - 2 * self.l2 * l.b
</code></pre>
<p>```</p>
<h2 id="training-mnist-in-batches-using-mse">Training MNIST in Batches using MSE</h2>
<p>In the code below, we create a training/validation loop. Each important point is commented. </p>
<p>```python
from tqdm import auto
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split</p>
<p>def to_one_hot(vector: Tensor) -&gt; Tensor:
    """Create one hot encoding of a vector."""
    oh = np.zeros((vector.shape[0], vector.max()+1))
    oh[np.arange(vector.shape[0]), vector] = 1
    return oh</p>
<h1 id="load-training-data">Load training data</h1>
<p>train = pd.read_csv('mnist_train.csv', header=None).values[:, 1:]
train_label = pd.read_csv(
    'mnist_train.csv', header=None).values[:, 0]</p>
<h1 id="create-the-basic-network">Create the basic network</h1>
<p>net = Network(layers=[
    Linear(784, 128),
    ReLU(),
    Linear(128, 10),
])</p>
<h1 id="initialize-loss-class">Initialize loss class</h1>
<p>loss = MSE()</p>
<h1 id="initialize-the-optimizer-learning-rate-is-00001">Initialize the optimizer, learning rate is 0.0001</h1>
<p>optim = SGD(1e-4)</p>
<h1 id="permform-the-trainval-split">permform the train/val split</h1>
<p>x_train, x_val, y_train, y_val = train_test_split(
    train.astype(np.float32) / 255,
    train_label.astype(np.int32),
    test_size=0.2, random_state=42)  # to_one_hot</p>
<h1 id="convert-labels-to-one-hot-encodings">Convert labels to one-hot encodings</h1>
<p>y_train = to_one_hot(y_train)
y_val = to_one_hot(y_val)</p>
<h1 id="batch-size">batch size</h1>
<p>batch_size = 100</p>
<h1 id="progress-bar-may-not-be-visible-in-pdf-mode-but-it-works-in-notebook-or-terminal-mode">progress bar may not be visible in PDF mode, but it works in notebook or terminal mode</h1>
<h1 id="we-set-it-to-100-epochs-here">we set it to 100 epochs here</h1>
<p>progress_bar = auto.tqdm(range(100))
for epoch in progress_bar:
    # offset to iterate through batches
    offset = 0
    # initialize errors for validation and training
    val_err = 0
    err = 0
    while (offset+batch_size &lt;= len(x_train)):
        # while we can move through batches, extract them
        data = x_train[offset:offset+batch_size, :]
        label = y_train[offset:offset+batch_size]
        # make prediction
        pred = net(data)
        # calculate loss (and average error)
        err += loss(pred, label)/(len(x_train)/batch_size)
        # begin backprop
        g = net.backward(loss.grad)
        # perform SGD step
        optim.step(net)
        # move to next batch
        offset += batch_size
    # reset offset for validation
    offset = 0
    while (offset+batch_size &lt;= len(x_val)):
        # get validation data while we are not at the end
        val_data = x_val[offset:offset+batch_size, :]
        val_label = y_val[offset:offset+batch_size]
        # make prediction
        pred = net(val_data)
        # get loss and error
        val_err += loss(pred, val_label)/(len(x_val)/batch_size)
        # move offset to next batch
        offset += batch_size
        if (epoch) % 2 == 0:
            # update progress bar info
            progress_bar.set_postfix({"Mean_loss_train": err,
                                      "Mean_loss_val": val_err})
```</p>
<p>```python</p>
<h1 id="load-test-data-and-convert-to-one-hot">Load test data and convert to one-hot</h1>
<p>test = pd.read_csv('mnist_test.csv', header=None).values[:, 1:]
test_label = pd.read_csv('mnist_test.csv', header=None).values[:, 0]
test_label = to_one_hot(test_label)</p>
<h1 id="place-offset-and-initialize-error-to-0">place offset and initialize error to 0</h1>
<p>offset = 0
test_err = 0.
while (offset+batch_size &lt;= len(test)):
    # get data batch
    data = test[offset:offset+batch_size, :]
    label = test_label[offset:offset+batch_size]
    # make prediction
    pred = net(data)
    # get error
    test_err += loss(pred, label)/(len(test)/batch_size)
    offset += batch_size</p>
<p>print(f"Test Error is {test_err:.2f} ...")
```</p>
<pre><code>Test Error is 2643.26 ...
</code></pre>
<p>```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import set_matplotlib_formats
set_matplotlib_formats('retina')
plt.style.use('ggplot')
%matplotlib inline</p>
<p>y_true = test_label.argmax(1)
y_pred = net(test).argmax(1)
ax = plt.figure(figsize=(15, 7))
ax = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=".3f")
ax.set_xlabel("True")
ax.set_ylabel("Predicted")
```</p>
<pre><code>Text(114.0, 0.5, 'Predicted')
</code></pre>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-06-14-numpy-learn/output_17_1.png" /></p>
<p>We can see from the confusion matrix above that the model performs poorly if the training is based on MSE. Let us try a different loss function: Cross Entropy loss.</p>
<h2 id="cross-entropy-training">Cross Entropy Training</h2>
<p>```python
"""Training example for a simple network with MNIST Dataset."""
from tqdm import auto
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from datatype import Tensor</p>
<p>def to_one_hot(vector: Tensor) -&gt; Tensor:
    """Create one hot encoding of a vector."""
    oh = np.zeros((vector.shape[0], vector.max()+1))
    oh[np.arange(vector.shape[0]), vector] = 1
    return oh</p>
<h1 id="load-training-data_1">Load training data</h1>
<p>train = pd.read_csv('mnist_train.csv', header=None).values[:, 1:]
train_label = pd.read_csv(
    'mnist_train.csv', header=None).values[:, 0]</p>
<h1 id="create-the-network">Create the network</h1>
<p>net = Network(layers=[
    Linear(784, 128),
    Sigmoid(),
    Linear(128, 10),
])</p>
<h1 id="initialize-loss-class_1">Initialize loss class</h1>
<p>loss = CrossEntropyLoss()</p>
<h1 id="initialize-optimizer-with-regularization">Initialize optimizer with regularization</h1>
<p>optim = SGD(5e-2, 0.0001)</p>
<h1 id="split">split</h1>
<p>x_train, x_val, y_train, y_val = train_test_split(
    train.astype(np.float32) / 255,
    train_label.astype(np.int32),
    test_size=0.2, random_state=42)  # to_one_hot</p>
<h1 id="to-one-hot">to one-hot</h1>
<p>y_train = to_one_hot(y_train)
y_val = to_one_hot(y_val)
batch_size = 100
progress_bar = auto.tqdm(range(200))</p>
<h1 id="this-will-be-used-later">this will be used later</h1>
<p>accuracies: dict = {"train": [],
                    "val": [],
                    "test": []}
acc_train: list = []
acc_val: list = []</p>
<p>for epoch in progress_bar:
    offset = 0
    val_err = 0
    err = 0
    while (offset+batch_size &lt;= len(x_train)):
        # grab the batch
        data = x_train[offset:offset+batch_size, :]
        label = y_train[offset:offset+batch_size, :]
        # I try to avoid a runtime warning (only happens in notebook, not sure why)
        try:
            pred = net(data)
        except RuntimeWarning:
            print(f"Runtime warning on {offset}")
        # get loss
        err += loss(pred, label)/(len(x_train)/batch_size)
        # backprop
        g = net.backward(loss.grad)
        # update weights
        optim.step(net)
        # next batch index
        offset += batch_size
        # keep scores
        acc_train.append(accuracy_score(
            label.argmax(axis=1),
            pred.argmax(axis=1)
        ))
    offset = 0
    while (offset+batch_size &lt;= len(x_val)):
        # get validation data
        val_data = x_val[offset:offset+batch_size, :]
        val_label = y_val[offset:offset+batch_size]
        # predict
        pred = net(val_data)
        # get loss
        val_err += loss(pred, val_label)/(len(x_val)/batch_size)
        # next batch index
        offset += batch_size
        # keep scores
        acc_val.append(accuracy_score(
            val_label.argmax(axis=1),
            pred.argmax(axis=1)
        ))
    if (epoch) % 2 == 0:
        # update progress bar
        progress_bar.set_postfix({"loss_train": err,
                                  "loss_val": val_err,
                                  "acc_val": np.mean(acc_val)})
    # keep scores for visualization
    accuracies['train'].append(np.mean(acc_train))
    accuracies['val'].append(np.mean(acc_val))
    acc_train = []
    acc_val = []</p>
<h1 id="load-test-data-and-convert-to-one-hot_1">Load test data and convert to one-hot</h1>
<p>test = pd.read_csv('mnist_test.csv', header=None).values[:, 1:]
test_label = to_one_hot(pd.read_csv(
    'mnist_test.csv',
    header=None).values[:, 0])</p>
<p>offset = 0
test_err = 0.
while (offset+batch_size &lt;= len(test)):
    # get batch
    data = test[offset:offset+batch_size, :]
    label = test_label[offset:offset+batch_size]
    # predict
    pred = net(data)
    # get loss
    test_err += loss(pred, label)/(len(test)/batch_size)
    # offset
    offset += batch_size
    # get scores
    accuracies['test'].append(accuracy_score(
        label.argmax(axis=1),
        pred.argmax(axis=1)
    ))</p>
<p>print(f"Average Test Accuracy: {np.mean(accuracies['test']):.2f}")</p>
<p>```</p>
<pre><code>Average Test Accuracy: 0.95
</code></pre>
<p>Let us plot the evolution of accuracies during testing and confusion matrix. For a higher performing model we expect to see the confusion matrix consolidate results on the diagonal:</p>
<p><code>python
fig = plt.figure(figsize=(15, 6))
_ = plt.plot(accuracies['train'], label="Training score")
_ = plt.plot(accuracies['val'], label="Validation score")
_ = plt.xlabel("Epoch")
_ = plt.ylabel("Accuracy")
_ = plt.title("Accuracy per epoch")
_ = plt.legend()</code></p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-06-14-numpy-learn/output_22_0.png" /></p>
<p>```python
y_true = test_label.argmax(1)
y_pred = net(test).argmax(1)
_ = plt.figure(figsize=(15, 7))
ax = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=".3f")</p>
<p><em>=ax.set_xlabel("True")
</em>=ax.set_ylabel("Predicted")
```</p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-06-14-numpy-learn/output_23_1.png" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>We implemented a neural network class that supports several activation functions. We followed here a design pattern based on PyTorch deep learning package. We implemented linear (fully-connected) layer, ReLU and Sigmoid layer. Each layer includes a backpropagation function <code>backward</code> that sends the gradient from the output back to input. As a result we were able to use a Cross Entropy Loss function to train a handwritten digit classifier with 95% accuracy on the test set. Notice that on the graph we observe a pattern of periodically dropping accuracy. I assume this is due to internal structure of the loss landscape: we repeatedly "walk" out of the minimum region and then "walk" back in during the SGD.</p>
<p>Using the Mean Squared Error loss function did not yield a productive result here, however, while developing this library, I observed that if I train on a small sample of data (i.e. 50 items or less), the model was able to learn the underlying data representations very well and was able to over fit. This is one the tests usually performed on new architectures in order to check if the model can learn at all. This, unfortunately, did not scale in case of MSE but it did for Cross Entropy Loss.</p>
<p>The overall structure of the project is posted on my github <a href="https://github.com/iliailmer/numpy_learn">page</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://iliailmer.github.io/tag/machine-learning.html">machine learning</a>
      <a href="https://iliailmer.github.io/tag/python.html">python</a>
      <a href="https://iliailmer.github.io/tag/numpy.html">numpy</a>
      <a href="https://iliailmer.github.io/tag/deep-learning.html">deep learning</a>
    </p>
  </div>





</article>

    <footer>
<p>
  &copy; 2021  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://iliailmer.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="light"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Ilia Ilmer ",
  "url" : "https://iliailmer.github.io",
  "image": "https://iliailmer.github.io/images/compressed.jpeg",
  "description": ""
}
</script>

</body>

</html>