
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index, follow" />

  <link
    href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/stylesheet/style.min.css">

  <link id="dark-theme-style" rel="stylesheet" type="text/css"   media="(prefers-color-scheme: dark)"      href="https://iliailmer.github.io/theme/stylesheet/dark-theme.min.css">

  <link id="pygments-dark-theme" rel="stylesheet" type="text/css"      media="(prefers-color-scheme: dark)"      href="https://iliailmer.github.io/theme/pygments/monokai.min.css">
  <link id="pygments-light-theme" rel="stylesheet" type="text/css"     media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"      href="https://iliailmer.github.io/theme/pygments/emacs.min.css">

  <link rel="stylesheet" href="https://iliailmer.github.io/theme/tipuesearch/tipuesearch.min.css" />

  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://iliailmer.github.io/theme/font-awesome/css/solid.css">


  <link href="https://iliailmer.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
    title="Ilia Ilmer Atom">


  <link rel="shortcut icon" href="https://iliailmer.github.io/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="https://iliailmer.github.io/images/favicon.ico" type="image/x-icon">

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-131502498-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

  <!-- Chrome, Firefox OS and Opera -->
  <meta name="theme-color" content="#333">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#333">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Microsoft EDGE -->
  <meta name="msapplication-TileColor" content="#333">


<meta name="author" content="Ilia Ilmer" />
<meta name="description" content="In this post I wanted to describe a simple application of a linear least squares method to a problem of data classification. It is a naive approach and is unlikely to beat more sophisticated techniques like Logistic Regression, for instance. Imports Some imports we are going to need for this …" />
<meta name="keywords" content="machine learning, linear regression, python, scikit-learn, statistical learning">


  <meta property="og:site_name" content="Ilia Ilmer"/>
  <meta property="og:title" content="Linear Regression as the Simplest Classifier"/>
  <meta property="og:description" content="In this post I wanted to describe a simple application of a linear least squares method to a problem of data classification. It is a naive approach and is unlikely to beat more sophisticated techniques like Logistic Regression, for instance. Imports Some imports we are going to need for this …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://iliailmer.github.io/2020/02/data-explore.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2020-02-24 00:00:00-05:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://iliailmer.github.io/author/ilia-ilmer.html">
  <meta property="article:section" content="Posts"/>
  <meta property="article:tag" content="machine learning"/>
  <meta property="article:tag" content="linear regression"/>
  <meta property="article:tag" content="python"/>
  <meta property="article:tag" content="scikit-learn"/>
  <meta property="article:tag" content="statistical learning"/>
  <meta property="og:image" content="https://iliailmer.github.io/images/compressed.jpeg">

  <title>Ilia Ilmer &ndash; Linear Regression as the Simplest Classifier</title>

</head>

<body >
  <aside>
    <div>
      <a href="https://iliailmer.github.io/">
        <img src="https://iliailmer.github.io/images/compressed.jpeg" alt="Ilia Ilmer" title="Ilia Ilmer">
      </a>

      <h1>
        <a href="https://iliailmer.github.io/">Ilia Ilmer</a>
      </h1>

<p>Algorithms and Coffee</p>
      <form class="navbar-search" action="https://iliailmer.github.io/search.html" role="search">
        <input type="text" name="q" id="tipue_search_input" placeholder="Search...">
      </form>

      <nav>
        <ul class="list">


          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/about.html#about">
              About
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/publications.html#publications">
              Publications
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/software.html#software">
              Software
            </a>
          </li>
          <li>
            <a target="_self"
              href="https://iliailmer.github.io/pages/talks.html#talks">
              Talks
            </a>
          </li>

        </ul>
      </nav>

      <ul class="social">
        <li>
          <a  class="sc-github" href="https://github.com/iliailmer" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        </li>
        <li>
          <a  class="sc-gitlab" href="https://gitlab.com/iliailmer" target="_blank">
            <i class="fab fa-gitlab"></i>
          </a>
        </li>
        <li>
          <a  class="sc-linkedin" href="https://linkedin.com/in/iilmer" target="_blank">
            <i class="fab fa-linkedin"></i>
          </a>
        </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="https://iliailmer.github.io/">Home</a>

      <a href="/files/resume.pdf">CV</a>

      <a href="https://iliailmer.github.io/feeds/all.atom.xml">Atom</a>

    </nav>

<article class="single">
  <header>
    
    <h1 id="data-explore">Linear Regression as the Simplest Classifier</h1>
    <p>
      Posted on Mon 24 February 2020 in <a href="https://iliailmer.github.io/category/posts.html">Posts</a>

    </p>
  </header>


  <div>
    <p>In this post I wanted to describe a simple application of a linear least squares method to a problem of data classification. It is a naive approach and is unlikely to beat more sophisticated techniques like <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>, for instance.</p>
<h3 id="imports">Imports</h3>
<p>Some imports we are going to need for this piece.</p>
<p>```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
%matplotlib inline</p>
<p>plt.style.use("ggplot")
```</p>
<h3 id="prepare-the-data">Prepare the data</h3>
<p>Data can be found <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">here</a>. The link also has information about the textbook with excellent theoretical background on much of Machine Learning (and Computational Statistics).</p>
<p>The data is a collection of hand-written digits. Each row of the data represents pixels of the image. For simplicity, we discard all labels except 2 and 3 making the problem a binary classification one (since only two labels are involved).</p>
<p>We load files with space character as a separator and no header</p>
<p><code>python
train = pd.read_csv('zip.train', sep=' ', header=None).drop(257, axis=1)
test = pd.read_csv('zip.test', sep=' ', header=None)</code></p>
<p>Select only required labels: (by default labels are read as <code>float64</code> but we won't worry about that) </p>
<p><code>python
train = train.loc[train[0].isin([2.0, 3.0])].reset_index(drop=True)
test = test.loc[test[0].isin([2.0, 3.0])].reset_index(drop=True)</code></p>
<p>```python</p>
<h1 id="convert-types-to-int32-and-replace-labels-with-0-and-1">convert types to int32 and replace labels with 0 and 1</h1>
<p>train[0] = train[0].astype(np.int32).map({2: 0, 3: 1})
test[0] = test[0].astype(np.int32).map({2: 0, 3: 1})</p>
<p>X, y = train.iloc[:, 1:], train.iloc[:, 0]
X_test, y_test = test.iloc[:, 1:], test.iloc[:, 0]
```</p>
<p><code>python
X.info()</code></p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1389 entries, 0 to 1388
Columns: 256 entries, 1 to 256
dtypes: float64(256)
memory usage: 2.7 MB
</code></pre>
<p><code>python
y.describe()</code></p>
<pre><code>count    1389.000000
mean        0.473722
std         0.499489
min         0.000000
25%         0.000000
50%         0.000000
75%         1.000000
max         1.000000
Name: 0, dtype: float64
</code></pre>
<p><code>python
X_test.info()</code></p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 364 entries, 0 to 363
Columns: 256 entries, 1 to 256
dtypes: float64(256)
memory usage: 728.1 KB
</code></pre>
<p><code>python
y_test.describe()</code></p>
<pre><code>count    364.000000
mean       0.456044
std        0.498750
min        0.000000
25%        0.000000
50%        0.000000
75%        1.000000
max        1.000000
Name: 0, dtype: float64
</code></pre>
<h4 id="some-helper-functions">Some helper functions</h4>
<p>```python
def show_sample(data: pd.DataFrame):
    """Show 4 images from the dataset (random)."""
    sample = data.sample(4)
    fig, ax = plt.subplots(2, 2, figsize=(10, 8))
    for i in range(2):
        for j in range(2):
            ax[i, j].imshow(sample.iloc[2 * i + j, 1:].values.reshape(16, 16))
            ax[i, j].axis('off')
            ax[i, j].set_title(f'Label: {sample.iloc[2*i+j, 0]}')
    plt.show()</p>
<p>def raw2label(y_pred, threshold=0):
    """Convert raw label into a int label with a threshold."""
    return np.int32(y_pred &gt; threshold)
```</p>
<p><code>python
show_sample(test)</code></p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-02-24-data-explore/output_18_0.png" /></p>
<h3 id="linear-regression-fitting">Linear Regression fitting</h3>
<p>We will approach the solution in the following way. Firstly, we will train a linear regression model on the available data (256 pixel intensity values). Note that the problem is high-dimensional and therefore it is impossible to visualize all of it at once.</p>
<p>However, the main idea of the linear regression based classifier remains simple: we find a hyperplane in the <span class="math">\(p+1=257\)</span> dimensional space that fits the training data perfectly:</p>
<div class="math">$$ \hat{y}=\mathbf{X}\hat{\beta},$$</div>
<p>where <span class="math">\(X_i\)</span> is the input column vector (features) with intercept (that is <span class="math">\(X_0=1\)</span>):</p>
<div class="math">$$\\X_i=(1, X_{i1} ,\dots X_{ip} )^T \in \mathbb{R}^{p+1},$$</div>
<p><span class="math">\(\mathbf{X}\)</span> is the matrix of features</p>
<div class="math">$$\\\mathbf{X} = [X_1^T,\dots, X_n^T]\in \mathbb{R}^{n\times(p+1)}\text{},$$</div>
<p>and <span class="math">\(\beta=[\beta_0, \dots, \beta_p]\)</span> is the vector of parameters</p>
<div class="math">$$\\\beta=[\beta_0, \dots, \beta_p]\in \mathbb{R}^{p+1},$$</div>
<p>where <span class="math">\(\hat{\beta}\)</span> is given by </p>
<div class="math">$$ \hat{\beta} = (X^TX)^{-1}X^Ty.$$</div>
<p>We are projecting our training set <span class="math">\(\mathbf{X}\)</span> orthogonally onto the hyperplane defined by <span class="math">\(\hat\beta\)</span>. We can then derive a classifier from there as follows: let us set up the threshold <span class="math">\(t\)</span> such that if for a given unseen feature vector <span class="math">\(X_{test}\)</span> the value <span class="math">\(y_{test}=X_{test}^T\hat\beta=\hat\beta_0+\sum\limits_{i=1}^p (X_{test})_i\hat\beta_i\)</span> is greater than <span class="math">\(t\)</span>, we assign label 1, otherwise 0.</p>
<p><code>python
linreg = LinearRegression()
linreg.fit(X, y)</code></p>
<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</code></pre>
<h4 id="train-set-predictions">Train set predictions</h4>
<p><code>python
y_true = y.copy().values
y_pred = linreg.predict(X)</code></p>
<p>We select the threshold range between the minimum and maximum value of the predicted <span class="math">\(y\)</span> with a step 0.1 and then check the accuracy for each threshold.</p>
<p>```python
thresh_range = np.arange(y_pred.min(), y_pred.max(), 0.1)
train_acc_scores = []</p>
<p>for t in thresh_range:
    train_acc_scores.append(
        accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true))
```</p>
<p>```python
fig, ax = plt.subplots(ncols=2, figsize=(24, 4))
ax[0].plot(thresh_range, train_acc_scores, '-o')
ax[0].set_title("Accuracy on Train Set")
ax[0].set_xlabel("Threshold")
ax[0].set_ylabel("Accuracy score")
ax[0].set_ylim([0.4, 1.05])
max_score = max(train_acc_scores)
max_loc = np.argmax(train_acc_scores)
ax[0].annotate(
    f'Max Accuracy: {max_score:.3f}',
    xy=(thresh_range[max_loc], max_score),
    xytext=(thresh_range[max_loc], max_score + 0.025),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>ax[1].plot(thresh_range, 1 - np.array(train_acc_scores), '-o')
ax[1].set_title("Error Train Set")
ax[1].set_xlabel("Threshold")
ax[1].set_ylabel("Error")
min_score = min(1 - np.array(train_acc_scores))
min_loc = np.argmin(1 - np.array(train_acc_scores))
ax[1].annotate(
    f'Min Error: {min_score:.3f}',
    xy=(thresh_range[min_loc], min_score),
    xytext=(thresh_range[min_loc], min_score + 0.025),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>fig.text(
    0.5,
    -0.1,
    'As the threshold increases we observe simultaneous increase in accuracy.\n'+\
    'As we cross the maximum accuracy, we essentially shift the hyperplane away from the data and thus see the decrease.',
    ha='center',
    fontsize=16);</p>
<p>```</p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-02-24-data-explore/output_26_0.png" /></p>
<h4 id="test-set-predictions">Test set predictions</h4>
<p>We can do similar analysis on the testing set.</p>
<p><code>python
y_true = y_test.copy().values
y_pred = linreg.predict(X_test)
test_acc_scores = []
for t in thresh_range:
    test_acc_scores.append(
        accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true))</code></p>
<p>```python
fig, ax = plt.subplots(ncols=2, figsize=(24, 6))
ax[0].plot(thresh_range, test_acc_scores, '-o')
ax[0].set_title("Accuracy on Test Set")
ax[0].set_ylim([0.4, 1.05])</p>
<h1 id="ax0set_xticksthresh_range">ax[0].set_xticks(thresh_range)</h1>
<p>ax[0].set_xlabel("Threshold")
ax[0].set_ylabel("Accuracy score")</p>
<p>max_score = max(test_acc_scores)
max_loc = np.argmax(test_acc_scores)
ax[0].annotate(
    f'Max Accuracy: {max_score:.3f}',
    xy=(thresh_range[max_loc], max_score),
    xytext=(thresh_range[max_loc] + 0.05, max_score + 0.05),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>ax[1].plot(thresh_range, 1 - np.array(test_acc_scores), '-o')
ax[1].set_title("Error Test Set")
ax[1].set_xlabel("Threshold")
ax[1].set_ylabel("Error")
ax[1].set_ylim([0, 0.55])
min_score = min(1 - np.array(test_acc_scores))
min_loc = np.argmin(1 - np.array(test_acc_scores))
ax[1].annotate(
    f'Min Error: {min_score:.3f}',
    xy=(thresh_range[min_loc], min_score),
    xytext=(thresh_range[min_loc] - 0.15, min_score - 0.03),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>fig.text(
    0.5,
    -0.1,
    'We observe a similar behaviour as the threshold increases.\n'+\
    'Notice that we use the same threshold as in the test set. '+\
    'The graphs converge around random guesses.',
    ha='center',
    fontsize=16);
```</p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-02-24-data-explore/output_30_0.png" /></p>
<p><code>python
percentage = np.unique(y_test, return_counts=True)[1]/len(y_test) # proportion of each label in the training set
print(f'Minimal accuracy on the left: {min(test_acc_scores)}, proportion of "0" labels in the data: {percentage[0]}')
print(f'Minimal accuracy on the right: {test_acc_scores[-1]}, proportion of "1" labels in the data: {percentage[1]}')</code></p>
<pre><code>Minimal accuracy on the left: 0.46153846153846156, proportion of "0" labels in the data: 0.5439560439560439
Minimal accuracy on the right: 0.5467032967032966, proportion of "1" labels in the data: 0.45604395604395603
</code></pre>
<p>From the information above we learn that the model start to approximately randomly guess once the thresholds are too big or too small. On the left in is closer to guessing ones, on the right - zeros.</p>
<p><code>python
print(f"Labels from leftmost threshold:\n{raw2label(y_pred, threshold=thresh_range[0])}")
print(f"\n\nLabels from rightmost threshold:\n{raw2label(y_pred, threshold=thresh_range[-1])}")</code></p>
<pre><code>Labels from leftmost threshold:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]


Labels from rightmost threshold:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
</code></pre>
<p>Let us use different thresholds for the testing set:</p>
<p><code>python
y_true = y_test.copy().values
y_pred = linreg.predict(X_test)
thresh_range = np.arange(y_pred.min(), y_pred.max(), 0.1)
test_acc_scores = []
for t in thresh_range:
    test_acc_scores.append(
        accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true))</code></p>
<p>```python
fig, ax = plt.subplots(ncols=2, figsize=(24, 6))
ax[0].plot(thresh_range, test_acc_scores, '-o')
ax[0].set_title("Accuracy on Test Set")
ax[0].set_ylim([0.4, 1.05])</p>
<h1 id="ax0set_xticksthresh_range_1">ax[0].set_xticks(thresh_range)</h1>
<p>ax[0].set_xlabel("Threshold")
ax[0].set_ylabel("Accuracy score")</p>
<p>max_score = max(test_acc_scores)
max_loc = np.argmax(test_acc_scores)
ax[0].annotate(
    f'Max Accuracy: {max_score:.3f}',
    xy=(thresh_range[max_loc], max_score),
    xytext=(thresh_range[max_loc] + 0.05, max_score + 0.05),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>ax[1].plot(thresh_range, 1 - np.array(test_acc_scores), '-o')
ax[1].set_title("Error Test Set")
ax[1].set_xlabel("Threshold")
ax[1].set_ylabel("Error")
ax[1].set_ylim([0, 0.55])
min_score = min(1 - np.array(test_acc_scores))
min_loc = np.argmin(1 - np.array(test_acc_scores))
ax[1].annotate(
    f'Min Error: {min_score:.3f}',
    xy=(thresh_range[min_loc], min_score),
    xytext=(thresh_range[min_loc] - 0.15, min_score - 0.03),
    arrowprops=dict(facecolor='black', shrink=0.01),
)</p>
<p>fig.text(
    0.5,
    -0.1,
    'We observe a similar behaviour as the threshold increases.\n'+\
    'Notice that we now see convergence to a random guess clearer. '+\
    'This is due to the threshold being intrinsic to the test set.',
    ha='center',
    fontsize=16);
```</p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-02-24-data-explore/output_36_0.png" /></p>
<p>```python
percentage = np.unique(y_test, return_counts=True)[1] / len(
    y_test)  # proportion of each label in the training set
print(
    f'Minimal accuracy on the left: {min(test_acc_scores)},'
    + f' proportion of "0" labels in the data: {percentage[0]}'
)
print(
    f"Labels from leftmost threshold:\n{raw2label(y_pred, threshold=thresh_range[0])}"
)</p>
<p>print(
    f'\n\nMinimal accuracy on the right: {test_acc_scores[-1]},' +
    f' proportion of "1" labels in the data: {percentage[1]}'
)
print(
    f"Labels from rightmost threshold: \n{raw2label(y_pred, threshold=thresh_range[-1])}"
)
```</p>
<pre><code>Minimal accuracy on the left: 0.45879120879120877, proportion of "0" labels in the data: 0.5439560439560439
Labels from leftmost threshold:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]


Minimal accuracy on the right: 0.5412087912087912, proportion of "1" labels in the data: 0.45604395604395603
Labels from rightmost threshold: 
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]
</code></pre>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<p>For this method, we consider different values of neighbors in each case. We begin with 1 neighbor per training point, which we expect to give a high accuracy since each point will roughly be each own neighborhood.</p>
<p>As the number of neighbors increases we will see a slight decrease in accuracy.</p>
<p>```python
train_err_scores = []
test_err_scores = []</p>
<p>for k in [1, 3, 5, 7, 15]:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X, y)</p>
<pre><code>y_true = y.copy().values
y_pred = knn.predict(X)

train_err_scores.append(1 - accuracy_score(y_pred=y_pred, y_true=y_true))

y_true = y_test.copy().values
y_pred = knn.predict(X_test)
test_err_scores.append(1 - accuracy_score(y_pred=y_pred, y_true=y_true))
</code></pre>
<p>```</p>
<p>```python
fig, ax = plt.subplots(ncols=2, figsize=(24, 6))
ax[1].plot([1, 3, 5, 7, 15], train_err_scores, '-o')
ax[1].set_title("Error on Train Set")
ax[1].set_xlabel("Neighbors")
ax[1].set_ylabel("Error score")</p>
<p>ax[0].plot([1, 3, 5, 7, 15], test_err_scores, '-o')
ax[0].set_title("Error on Test Set")
ax[0].set_xlabel("Neighbors")
ax[0].set_ylabel("Error score")</p>
<p>fig.text(
    0.5,
    -0.1,
    'As the number of neighbors increases, so does the error score of the model on the training data.',
    ha='center',
    fontsize=16);
```</p>
<p><img alt="png" src="https://iliailmer.github.io/images/2020-02-24-data-explore/output_41_0.png" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>We applied two different classification methods for the data. Method 1 was based on the linear regression using least squares method. The second method was based on 5 different Nearest neighbor classifiers.</p>
<p>Each approach showed different results specific to assumptions underlying each model. Linear model assumes that the relationship between the label and pixel intensities is a high-dimensional linear function. Using various threshold we were able to show the change in hyperplane location relative to test and train data and how the accuracy scores (error scores) reflect this change.</p>
<p>For the k-NN model, we observed a similar behavior: going from few neighbors to more, we saw that the model is prone to the larger error. This is to be expected but not to say that 1 neighbor is necessarily better than 15 neighbors, since 1 neighbor case is <em>overfitting</em> the data. The data is very high-dimensional so it is harder to evaluate how many neighbors we should aim for here.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://iliailmer.github.io/tag/machine-learning.html">machine learning</a>
      <a href="https://iliailmer.github.io/tag/linear-regression.html">linear regression</a>
      <a href="https://iliailmer.github.io/tag/python.html">python</a>
      <a href="https://iliailmer.github.io/tag/scikit-learn.html">scikit-learn</a>
      <a href="https://iliailmer.github.io/tag/statistical-learning.html">statistical learning</a>
    </p>
  </div>





</article>

    <footer>
<p>
  &copy; 2021  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://iliailmer.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="light"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Ilia Ilmer ",
  "url" : "https://iliailmer.github.io",
  "image": "https://iliailmer.github.io/images/compressed.jpeg",
  "description": ""
}
</script>

</body>

</html>