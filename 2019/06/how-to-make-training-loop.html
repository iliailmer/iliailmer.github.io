<!DOCTYPE html>
<html lang="en">
<head>
          <title>Ilia Ilmer - How to write a decent training loop with enough flexibility.</title>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <link href="https://iliailmer.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Ilia Ilmer Full Atom Feed" />
        <link href="https://iliailmer.github.io/feeds/posts.atom.xml" type="application/atom+xml" rel="alternate" title="Ilia Ilmer Categories Atom Feed" />




    <meta name="tags" content="deep learning" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://iliailmer.github.io/">Ilia Ilmer <strong>Algorithms and Coffee</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="https://iliailmer.github.io/pages/about.html">About</a></li>
            <li><a href="https://iliailmer.github.io/pages/cv.html">CV</a></li>
            <li><a href="https://iliailmer.github.io/pages/publications.html">Publications</a></li>
            <li><a href="https://iliailmer.github.io/pages/software.html">Software</a></li>
            <li><a href="https://iliailmer.github.io/pages/talks.html">Talks</a></li>
            <li class="active"><a href="https://iliailmer.github.io/category/posts.html">Posts</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://iliailmer.github.io/2019/06/how-to-make-training-loop.html" rel="bookmark"
         title="Permalink to How to write a decent training loop with enough flexibility.">How to write a decent training loop with enough flexibility.</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2019-06-15T00:00:00-04:00">
      Sat 15 June 2019
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://iliailmer.github.io/author/ilia-ilmer.html">Ilia Ilmer</a>
    </address>
    <div class="category">
        Category: <a href="https://iliailmer.github.io/category/posts.html">Posts</a>
    </div>
    <div class="tags">
        Tags:
            <a href="https://iliailmer.github.io/tag/deep-learning.html">deep learning</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p>In this post, I briefly describe my experience in setting up training with PyTorch.</p>
<h2>Introduction</h2>
<p>PyTorch is an extremely useful and convenient framework for deep learning. When it comes to working on  a deep learning project, I am more comfortable with PyTorch rather than TensorFlow.</p>
<p>In this quick post, I would like to show how one can go about building a custom training loop, something that I struggled when I was getting started. It is a useful skill to be able to build the training loop on your own because that can help you understand better what happens under the hood of a deep learning package that abstracts a lot of nuts and bolts away from the end-user.</p>
<h2>The Overview of Training</h2>
<p>When one trains a network, we need to follow a certain paradigm.</p>
<p>First, set the model into training mode.</p>
<p>Second, start iterating through the training set.</p>
<p>For every batch we must:</p>
<ul>
<li>compute the output of the network</li>
<li>compute the loss</li>
<li>get gradients</li>
<li>start descending using the optimizer</li>
</ul>
<p>This last step we acknowledge that the method for optimization of our model is based on a gradient descent. It can be eqither Adam, SGD, or any other (RAdam is the brand new one which seems to beat state of the art).</p>
<p>In code, we can put it in the form like this:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># preparing model for training</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">training_set</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>  <span class="c1"># unpack the batch</span>
    <span class="c1"># the step below is necessary so that we update the gradient only pertinent to the current batch</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># compute the output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the gradient using backpropagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># take a step with the optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3>A Trick for Better Training with Lower Memory</h3>
<p>A small batch can result in a small gradient. This, in turn, leads to a problem called <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>: the value is so small, computer simple treats it as zero (underflow). To avoid it, a trick of accumulating gradient as you iterate through the dataset. I saw a practical implementation in this <a href="https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614#latest-662360">discussion</a>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_accumulate</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">accumulation_step</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># preparing model for training</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">training_set</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>  <span class="c1"># unpack the batch</span>
    <span class="c1"># compute the output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the gradient using backpropagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">idx</span><span class="o">%</span><span class="n">accumulation_step</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="c1"># take a step with the optimizer once</span>
      <span class="c1"># we accumulated enough gradients</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h2>In Closing: abstracting training loop</h2>
<p>In this post I summarized my experience in building a training loop for PyTorch. Lately, I have been using a more abstracted way of training through <a href="https://catalyst-team.github.io/catalyst/">Catalyst</a>. It is a great tool for higher level abstraction during training and a lot of hardwork has been done to take away the hard part of training.</p>
<p>Nevertheless, both, I believe, are equally important: the abstract and the explicit methods.</p>
<p>Thanks for reading!</p>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>