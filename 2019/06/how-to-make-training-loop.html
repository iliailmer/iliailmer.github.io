
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="/theme/pygments/github.min.css">


  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/solid.css">

    <link href="/static/custom.css" rel="stylesheet">

    <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Ilia's Blog Atom">




    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#333">

<meta name="author" content="Ilia Ilmer" />
<meta name="description" content="In this post, I briefly describe my experience in setting up training with PyTorch. Introduction PyTorch is an extremely useful and convenient framework for deep learning. When it comes to working on a deep learning project, I am more comfortable with PyTorch rather than TensorFlow. In this quick post, I …" />
<meta name="keywords" content="deep learning">


<meta property="og:site_name" content="Ilia's Blog"/>
<meta property="og:title" content="&#34;How to write a decent training loop with enough flexibility.&#34;"/>
<meta property="og:description" content="In this post, I briefly describe my experience in setting up training with PyTorch. Introduction PyTorch is an extremely useful and convenient framework for deep learning. When it comes to working on a deep learning project, I am more comfortable with PyTorch rather than TensorFlow. In this quick post, I …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/2019/06/how-to-make-training-loop.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-06-15 00:00:00-04:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="/author/ilia-ilmer.html">
<meta property="article:section" content="Posts"/>
<meta property="article:tag" content="deep learning"/>
<meta property="og:image" content="">

  <title>Ilia's Blog &ndash; &#34;How to write a decent training loop with enough flexibility.&#34;</title>

</head>
<body class="light-theme">
  <aside>
    <div>
      <a href="">
        <img src="/theme/img/profile.png" alt="Ilia Ilmer" title="Ilia Ilmer">
      </a>

      <h1>
        <a href="">Ilia Ilmer</a>
      </h1>

<p>Algorithms and Coffee</p>

      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="/pages/about.html#about">
                  About
                </a>
              </li>

        </ul>
      </nav>

      <ul class="social">
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="">Home</a>


      <a href="/feeds/all.atom.xml">Atom</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="how-to-make-training-loop">"How to write a decent training loop with enough flexibility."</h1>
    <p>
      Posted on Sat 15 June 2019 in <a href="/category/posts.html">Posts</a>

    </p>
  </header>


  <div>
    <p>In this post, I briefly describe my experience in setting up training with PyTorch.</p>
<h2>Introduction</h2>
<p>PyTorch is an extremely useful and convenient framework for deep learning. When it comes to working on  a deep learning project, I am more comfortable with PyTorch rather than TensorFlow.</p>
<p>In this quick post, I would like to show how one can go about building a custom training loop, something that I struggled when I was getting started. It is a useful skill to be able to build the training loop on your own because that can help you understand better what happens under the hood of a deep learning package that abstracts a lot of nuts and bolts away from the end-user.</p>
<h2>The Overview of Training</h2>
<p>When one trains a network, we need to follow a certain paradigm.</p>
<p>First, set the model into training mode.</p>
<p>Second, start iterating through the training set.</p>
<p>For every batch we must:</p>
<ul>
<li>compute the output of the network</li>
<li>compute the loss</li>
<li>get gradients</li>
<li>start descending using the optimizer</li>
</ul>
<p>This last step we acknowledge that the method for optimization of our model is based on a gradient descent. It can be eqither Adam, SGD, or any other (RAdam is the brand new one which seems to beat state of the art).</p>
<p>In code, we can put it in the form like this:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># preparing model for training</span>
  <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">training_set</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>  <span class="c1"># unpack the batch</span>
    <span class="c1"># the step below is necessary so that we update the gradient only pertinent to the current batch</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># compute the output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the gradient using backpropagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># take a step with the optimizer</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3>A Trick for Better Training with Lower Memory</h3>
<p>A small batch can result in a small gradient. This, in turn, leads to a problem called <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>: the value is so small, computer simple treats it as zero (underflow). To avoid it, a trick of accumulating gradient as you iterate through the dataset. I saw a practical implementation in this <a href="https://www.kaggle.com/c/understanding_cloud_organization/discussion/105614#latest-662360">discussion</a>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_accumulate</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">accumulation_step</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># preparing model for training</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">training_set</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>  <span class="c1"># unpack the batch</span>
    <span class="c1"># compute the output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
    <span class="c1"># calculate the gradient using backpropagation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">idx</span><span class="o">%</span><span class="n">accumulation_step</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="c1"># take a step with the optimizer once</span>
      <span class="c1"># we accumulated enough gradients</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h2>In Closing: abstracting training loop</h2>
<p>In this post I summarized my experience in building a training loop for PyTorch. Lately, I have been using a more abstracted way of training through <a href="https://catalyst-team.github.io/catalyst/">Catalyst</a>. It is a great tool for higher level abstraction during training and a lot of hardwork has been done to take away the hard part of training.</p>
<p>Nevertheless, both, I believe, are equally important: the abstract and the explicit methods.</p>
<p>Thanks for reading!</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/deep-learning.html">deep learning</a>
    </p>
  </div>

  <div class="center social-share">
    <p>Like this article? Share it with your friends!</p>
    <div class="addthis_native_toolbox"></div>
    <div class="addthis_sharing_toolbox"></div>
    <div class="addthis_inline_share_toolbox"></div>
  </div>


    <div class="addthis_relatedposts_inline"></div>


</article>

    <footer>
<p>
  &copy; 2021  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-77hh6723hhjd" async="async"></script>


<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Ilia's Blog ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>


</body>
</html>