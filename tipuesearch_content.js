var tipuesearch = {"pages":[{"title":"About","text":"My name is Ilia, I am a computer science PhD student at City University of New York. I work on problems that arise in the area of parameter identifiability and symbolic computing. My other interests include, algorithms, data structures, machine learning and artificial intelligence. Here is my CV.","tags":"pages","url":"https://iliailmer.github.io/pages/about.html","loc":"https://iliailmer.github.io/pages/about.html"},{"title":"Publications","text":"List of Papers Web-based Structural Identifiability Analyzer , Ilia Ilmer, Alexey Ovchinnikov, Gleb Pogudin, to appear in Lecture Notes in Computer Science (LNCS), arXiv version can be found here ; On the interplay of harvesting and various diffusion strategies for spatially heterogeneous populations , Elena Braverman, Ilia Ilmer, Journal of Theoretical Biology, Vol 466, 2019, pp 106-118","tags":"pages","url":"https://iliailmer.github.io/pages/publications.html","loc":"https://iliailmer.github.io/pages/publications.html"},{"title":"Software","text":"List of Software Here is a list of programs I maintain, develop, and/or contribute to: I work on a Google Summer of Code project for SciML , contributing identifiability algorithm to the toolkit. We recently ported StructuralIdentifiability.jl which I help maintain. I develop and maintain Structural Identifiability Toolbox . An online tool for assessing parameter identifiability. I maintain SIAN-Julia , see dev fork here . I contributed to Symbolics.jl As a personal project, I wrote a numpy deep learning library in the style of PyTorch: numpy-learn I wrote a cli tool for gitignore file creation gitignore-create . Install it via pip install gitignore-create and run gitignore-create -n <your file type here> to get started! I am also a fan of Kaggle competitions for machine learning (especially love them when there is enough time for them!). For a list of our parameter identifiability software with links to source code, see here","tags":"pages","url":"https://iliailmer.github.io/pages/software.html","loc":"https://iliailmer.github.io/pages/software.html"},{"title":"Talks","text":"List of Talks 22nd Workshop on Computer Algebra in Memory of V. Gerdt , online, hosted at JINR, Dubna, Russia, May 24-25, 2021. Title : Software for structural identifiability analysis: role of Gröbner bases and characteristic sets; 4th International Conference \"Computer Algebra\" , online, hosted at RUDN, Moscow, Russia, June 28-29, 2021. Title : Role of monomial orderings in efficient Gröbner basis computation in parameter identifiability problem; The International Symposium on Symbolic and Algebraic Computation , online, hosted in Saint-Petersburg, Russia, July 18-23, 2021. Title : Maple Application for Structural Identifiability Analysis of ODE models; Award: Distinguished Software Demonstration Award 19th International Conference on Computational Methods in Systems Biology (upcoming), online, hosted in Bordeaux, France, September 22-24, 2021. Title : Web-based Structural Identifiability Analyzer; Maple Conference 2021 (upcoming), online, November 2-5, 2021","tags":"pages","url":"https://iliailmer.github.io/pages/talks.html","loc":"https://iliailmer.github.io/pages/talks.html"},{"title":"First Month In GSoC","text":"Project Updates We slightly churned our project idea from the original algorithm implementation into an inclusion of a StructuralIdentifiability.jl package develop by my colleague Gleb Pogudin . The package is currently part of SciML with more updates to come! Here is a list of things it can do: check local identifiability of parameters check global identifiability of parameters check for identifiable functions (work-in-progress feature) Future Directions We plan to add more tutorials, finalize registering the package, and make it compatible with ODE systems defined in other SciML projects!","tags":"Posts","url":"https://iliailmer.github.io/2021/07/first-month-in-gsoc.html","loc":"https://iliailmer.github.io/2021/07/first-month-in-gsoc.html"},{"title":"Structural Identifiability Toolbox","text":"Introduction In this repository, I will describe our recently-released Structural Identifiability Toolbox, a web-based application for assessing parameter identifiability of differential models. Click here to checkout the application! Read on to learn more. Why is it better? The program is fast, free, and is available in any web-browser, including mobile. We take care of a lot stuff in the background letting the user worry only about their model definition without any technicalities. Who is it designed for? Simply put, if you are working with models based on ordinary differential equations and you wish to know structural identifiability properties of your models' parameters then this app is built for you! Knowing structural identifiability properties will help one set up better experiments to obtain data for the underlying process and extract parameter values correctly. What can it do? Individual Parameters The application can answer questions about local or global identifiability parameters (including initial conditions): you provide the input system, choose the probability of correctness and, optionally, specify which parameter to check (by default it checks for all possible parameters). For this, the app is using SIAN algorithm[&#94;1][&#94;2] for details) which is fast, robust, and is correct with user-specified probability. Parameter Combinations If a parameter is non -identifiable, one may wish to seek an identifiable function that contains this parameter (and, possibly, others). Built with a brand new algorithm[&#94;3], the app can quickly assess generators for all such functions. Moreover, we provide a way to assess whether 1 or more experiments are required to do so, this is called multi-experiment identifiability. References [&#94;1]: H. Hong, A. Ovchinnikov, G. Pogudin, C. Yap, Global Identifiability of Differential Models , Communications on Pure and Applied Mathematics, Volume 73, Issue 9, Pages 1831-1879, 2020 [&#94;2]: H. Hong, A. Ovchinnikov, G. Pogudin, C. Yap, SIAN: Software for Structural Identifiability Analysis of ODE Models Bioinformatics, Volume 35, Issue 16, Pages 2873–2874, 2019 [3]: Computing All Identifiable Functions of ODE Models , arXiv:2004.07774","tags":"Posts","url":"https://iliailmer.github.io/2021/07/structural-identifiability-toolbox.html","loc":"https://iliailmer.github.io/2021/07/structural-identifiability-toolbox.html"},{"title":"My Google Summer of Code Project","text":"About The Project Problem Formulation The problem of parameter identifiability is one of the most crucial issues arising in systems biology. To take a look at a problem of identifiability, we must first describe a setting in which it arises. Systems biology deals with biological processes that are described by ordinary differential equations (ODEs). These equations, in essence, describe mathematically the model's evolution through time. Each such theoretical model can have dedicated inputs and outputs. An example of an input is a catalyst for chemical reaction. An example of an output could be a measurement done by the chemist. In addition to input and output, the model has states and parameters. States are quantities whose time evolution is being considered, while parameters are values that come from intrinsic properties of the system. For example, in a model of population growth such as the Lotka-Volterra model, a species' intrinsic growth rate is a parameter but the population density is a state. Now we are ready to pose the identifiability question: given an input and an output as functions of time, can we recover information about states and parameters? The answer to that question can be one of three kinds: No, the parameters or states of the system cannot be recovered from given inputs or outputs Yes, we can uniquely recover parameters and states Yes, we can recover parameters and states but up to finitely many values. Point 1 corresponds to non-identifiability. This means that an experiment with given inputs outputs cannot help the researcher recover the parameters or states. Answers 2 and 3 correspond to two kinds of identifiability: global and local respectively. The above problem formulation refers to structural identifiability, which is a theoretical property. Another type of identifiability is practical which studies recovery of parameters from given data. The Project This summer I will be working on an implementation of an algorithm for testing local (see part 3 above) identifiability which will become part of the ModelingToolkit.jl package. ModelingToolkit.jl and Symbolics.jl make a significant contribution to the area of symbolic computation for Julia language. It is currently an open problem to enhance these packages with the capabilities of structural identifiability analysis. This project consists of coding the algorithms for local and global identifiability problems using Julia Programming Language and Symbolics.jl with ModelingToolkit.jl . The general direction of solving this problem is to begin with fast and efficient (polynomial time) algorithm implementation. A good starting point is this algorithm for local identifiability tests that relies on power series solutions. Later on (and, more importantly, with enough time) we can enhance the functionality to allow tests for global identifiability checking.","tags":"Posts","url":"https://iliailmer.github.io/2021/06/my-google-summer-of-code-project.html","loc":"https://iliailmer.github.io/2021/06/my-google-summer-of-code-project.html"},{"title":"How I had to translate Matlab code into Maple","text":"In this short post, I wanted to point out one interesting application of regular expressions I had to work on for my PhD research project. The code was meant as a technical tool to help tranlate some ordingary differential equation models from numerical (Matlab) to symbolic (Maple) code. The original code The original *.m files were pulled from this webpage using wget and their own API. Ordingary differential equation (ODE) models in those files contain a special function called xdot . It returns an array of n elements, which form right-hand side of an ODE. The function is usually defined in the form m function xdot=f(x,t) % define parameters as C = 1.0; % ... xdot = zeros(..., ...); % define each xdot(i) separately end What I wanted to see in the maple code was the following: ```py define parameters as symbolic constants C := C: define system of odes as array sigma := [ diff(x1(t), t) = <...>, <...> diff(xn(t), t) = <...> ]: ``` The best way to solve it I saw was to use regular expressions. Step by step Firstly, I had to get rid of Matlab comment and turn them into Maple compatible ones, hence the line py out_program = re.findall( r\"function xdot=f\\(x,t\\)(.*?)end\", content, re.DOTALL )[0].replace(\"%\", \"#\") After that, we want to look at if statements. In Maple, conditional structure like if statements are written as pascal if <condition> then <code> end: To do that, we utilize groups: (..) . The regex is py out_program = re.sub( r\"if(\\(\\w*\\))(.*;)end\", r\"if \\1 then \\2\\nend if:\", out_program, flags=re.DOTALL, ) The if(\\(\\w*\\))(.*;)end regex gets the condition \\(\\w*\\) and the code (.*;) between if / end to place those between if , then , end in positions marked by \\1 and \\2 . Remeber, that we do not want to have xdot(i)=... assigned manually anymore in Maple, we want to see Maple syntax: diff(x(t),t) = ... , so we do the following regex: py out_program = re.sub( r\"xdot\\((\\d+)\\) \\=( .*);\", r\"\\ndiff(x\\1(t), t) = \\2,\", out_program ) Again, notice the groups that capture stuff we want to preserve, namely the index of xdot and the right-hand sides. Next few lines do some cosmetic work, namely, rename any x(i) appearance into xi(t) , then make all variable assingments symbolic constants (i.e. if we have c=0 we want to have c:=c ). Finally, if we have Matlab assignments that do not use constants (i.e. c=0; x = c+x; ) we want to keep them (i.e. c:=c: x:=c+x: ). py out_program = re.sub(r\"x\\((\\d+)\\)\", r\"x\\1(t)\", out_program) # x(i) -> xi(t) out_program = re.sub(r\"(\\w+)\\=\\d*\\..*\", r\"\\1:=\\1:\", out_program) # c=NUMBER -> c:=c: out_program = re.sub(r\"(\\w+)\\=(.*);\", r\"\\1:=\\2:\", out_program) # Left-side = ANYTHING NOT METNIONED ABOVE -> same but with := sign Finally, putting it all together we can run: ```py import re from glob import glob from tqdm.auto import tqdm files = glob(\"files/ / /*.m\") for i in tqdm(range(len(files))): with open(files[i], \"r\") as f: try: content = f.read() except: print(files[i], \"could not read\") try: out_program = re.findall( r\"function xdot=f(x,t)(. ?)end\", content, re.DOTALL )[0].replace(\"%\", \"#\") out_program = re.sub( r\"if((\\w ))(. ;)end\", r\"if \\1 then \\2\\nend if:\", out_program, flags=re.DOTALL, ) out_program = re.sub( r\"xdot((\\d+)) \\=( . );\", r\"\\ndiff(x\\1(t), t) = \\2,\", out_program ) out_program = re.sub(r\"(xdot=zeros.*)\", r\"# \\1\", out_program) # comment out declaration of xdot out_program = re.sub(r\"x\\((\\d+)\\)\", r\"x\\1(t)\", out_program) out_program = re.sub(r\"(\\w+)\\=\\d*\\..*\", r\"\\1:=\\1:\", out_program) out_program = re.sub(r\"(\\w+)\\=(.*);\", r\"\\1:=\\2:\", out_program) # cosmetic work to make maple run and not complain about trailing comma out_program = re.sub( r\"(diff.*)\", r\"sigma := [\\n\\1]:\", out_program, flags=re.DOTALL ).replace(\",\\n]\", \"\\n]\") outname = files[i].split(\".\")[0] + \".mpl\" with open(outname, \"w\") as output: output.write(out_program) import os os.system(\"mkdir -p new_examples\") os.system(f\"cp {outname} new_examples\") except: print(files[i], \"Index not Found.\") ``` Some stuff really specific to our project was omitted for brevity, but one can definitely add any other Maple/Matlab interaction here.","tags":"Posts","url":"https://iliailmer.github.io/2020/08/matlab-2-maple.html","loc":"https://iliailmer.github.io/2020/08/matlab-2-maple.html"},{"title":"NumPy-Learn, A Homemade Machine Learning Library","text":"In this post, I expand on a little class/self-teaching project that I did during the Spring 2020 semester. NumPy-Learn: A Homemade Machine Learning Library Organization In this section we will discuss the main organization of the library: How the layers are built How loss functions work How a stochastic gradient descent optimizer was built After that, we introduce the class for building the neural net itself and explain how everything ties together. We conclude by performance analysis on a simple MNIST program. Let us agree on convention similar to that of PyTorch library: we will call the main datatype Tensor instead of array , as follows: py from numpy import ndarray as Tensor This is to adhere to accepted aesthetics of most modern neural network libraries and nothing more. All methods are purely using numpy or scipy . Linear Layer Inspired by PyTorch, the naming convention here is the preserved. The design of the layer is also similar to PyTorch: the class Linear will have a forward and a backward methods. The former will represent the forward pass, that is, the passing of the input data through the layer towards the next. The latter is responsible for backward propagation of the gradient. Forward Pass Linear layer essentially represents matrix multiplication of the input data \\(x\\) by a weight matrix \\(W\\) with addition of bias \\(b\\) : $$\\mathrm{L}(x) = xW + b.$$ We require that the size of the input data was of the format \\(batch\\times input~features\\) , for instance if the input data has 784 pixel values, for a batch of 100 images the size of \\(x\\) would be \\(100\\times 784\\) and the size of \\(W\\) would be \\(784\\times out~features\\) , while \\(b\\) is of the shape \\(out~features\\times 1\\) . Here we rely on numpy broadcasting the value of bias onto the resulting matrix \\(xW\\) when adding \\(b\\) . In code, we define it as follows: py def forward(self, x: Tensor) -> Tensor: self.input = x return x @ self.W + self.b Gradient In the forward pass, we computed the matrix product. Next, we need to evaluate the rate of change of the output of the current layer with respect to the input. Note, that due to the chain rule, the gradient flows from right (output) to left (input) as a product. The backward method utilizes chainrule. It accepts the gradient from the layer \\(l+1\\) , uses it to find the derivatives of the current layer's output with respect to \\(W\\) and \\(b\\) and finally, passes it along multiplying by the derivative of its output w.r.t. \\(x\\) , the input. Mathematically, this is the following: The loss's derivatives w.r.t. weights are $$\\frac{\\partial E}{\\partial W&#94;{l}_{ij}}=\\sum\\limits_{\\text{input-output pair}}\\delta&#94;l_i out&#94;{l-1}_j$$ . Here, \\(\\delta&#94;l_i\\) is the error of the \\(l\\) th layer for \\(i\\) th node: $$\\delta&#94;l_i=g'_{out}(a_i&#94;l)\\sum\\limits_k W&#94;{l+1}_{ik}\\delta&#94;{l+1}_{k}$$ , where \\(g\\) is the activation function. These equations are written in a different shape convention, but we can take care of that in the code. The derivative of \\(xW+b\\) w.r.t. \\(W\\) is \\(x&#94;T\\) . The derivative of \\(xW+b\\) w.r.t. to \\(b\\) is an identity matrix. Therefore, let grad be the gradient (error) received from \\((l+1)\\) th layer, then we can define the backward method as below: py def backward(self, grad: Tensor) -> Tensor: # input_feat by batch_size @ batch_size by out_features self.dydw = self.input.T @ grad # we sum across batches and get shape (out_features) self.dydb = grad.sum(axis=0) # output must be of shape (batch_size, out_features) return grad @ self.W.T Now we are ready to present the fully defined Linear Layer code below: ```python import numpy as np from numpy import ndarray as Tensor class Linear: \"\"\"A linear layer.\"\"\" def __init__(self, in_features: int, out_features: int): \"\"\"Initialize a linear layer with weights and biases.\"\"\" self.W = np.random.randn(in_features, out_features) self.b = np.random.randn(out_features) def forward(self, x: Tensor) -> Tensor: \"\"\"Compute forward pass, return W @ x + b. Arguments: W: the weight Tensor of shape (in_featuers, out_features) b: the bias vector of shape (out_features,) x: the input of shape (batch_size, in_features) Returns: A tensor of shape (batch_size, out_features) \"\"\" self.input = x return x @ self.W + self.b def backward(self, grad: Tensor) -> Tensor: \"\"\"Propagate the gradient from the l+1 layer to l-1 layer. Arguments: grad: the tensor gradients from the l+1 layer to be propagated, shape: (batch_size, out_features). References: http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html \"\"\" # in_feat by batch_size @ batch_size by out_feat self.dydw = self.input.T @ grad # we sum across batches and get shape (out_features) self.dydb = grad.sum(axis=0) # output must be of shape (batch_size, out_features) return grad @ self.W.T def __call__(self, x: Tensor) -> Tensor: \"\"\"Peform forward pass on `__call__`.\"\"\" return self.forward(x) def __repr__(self) -> str: \"\"\"Print a representation for Jupyter/IPython.\"\"\" return f\"\"\"Linear Layer:\\n\\tWeight: {self.W.shape}\"\"\"\\ + f\"\"\"\\n\\tBias: {self.b.shape}\"\"\" ``` Layers with Activation Functions We define separate layers for activation functions, similarly to the way PyTorch handles those. We only define two here: ReLU and Sigmoid. ReLU is defined as $$f(x) = \\max\\{0, x\\}$$ and Sigmoid is defined as $$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$$ Their derivatives are defined as $$(\\nabla f)(t) = 0 \\text{ if } t=0\\text{, else }t$$ $$(\\nabla \\sigma)(t) = \\sigma(t)(1-\\sigma(t))$$ The respective classes are defined below ```python def sigmoid(x: Tensor) -> Tensor: \"\"\"Calculate the sigmoid function of x.\"\"\" return 1/(1+np.exp(-x)) def sigmoid_prime(x: Tensor) -> Tensor: \"\"\"Calculate the d/dx of sigmoid function of x.\"\"\" return sigmoid(x)*(1-sigmoid(x)) class ReLU: \"\"\"ReLU class.\"\"\" def __init__(self): \"\"\"Initialize the ReLU instance.\"\"\" def forward(self, x: Tensor) -> Tensor: \"\"\"Compute the activation in the forward pass. Arguments: x: Tensor of inputs, shape (batch_size, in_features) Returns: Tensor of shape (batch_size, in_features) \"\"\" return np.maximum(x, 0) def backward(self, grad: Tensor) -> Tensor: \"\"\"Compute the gradient and pass it backwards. Arguments: grad: Tensor of gradients of shape (batch_size, out_features) Returns: Tensor of shape (batch_size, out_features) \"\"\" return np.maximum(grad, 0) def __call__(self, x: Tensor) -> Tensor: \"\"\"Peform forward pass on `__call__`.\"\"\" return self.forward(x) def __repr__(self) -> str: \"\"\"Print a representation of ReLU for Jupyter/IPython.\"\"\" return \"\"\"ReLU()\"\"\" class Sigmoid: \"\"\"Sigmoid class.\"\"\" def __init__(self): \"\"\"Initialize the instance. We add the main function for activation and its derivative function. \"\"\" self.sigmoid = sigmoid self.sigmoid_prime = sigmoid_prime def forward(self, x: Tensor) -> Tensor: \"\"\"Compute the activation in the forward pass. Arguments: x: Tensor of inputs with shape(batch_size, in_features) Returns: Tensor of shape(batch_size, in_features) \"\"\" self.input = x return self.sigmoid(x) def backward(self, grad: Tensor): \"\"\"Compute the gradient and pass it backwards. Arguments: grad: Tensor of gradients with shape(batch_size, out_features) Returns: Tensor of shape(in_features, out_features) \"\"\" return self.sigmoid_prime(self.input) * grad def __call__(self, x: Tensor) -> Tensor: \"\"\"Peform forward pass on `__call__`.\"\"\" return self.forward(x) def __repr__(self) -> str: \"\"\"Print a representation of Sigmoid for Jupyter/IPython.\"\"\" return \"\"\"Sigmoid()\"\"\" ``` In addition to Sigmoid and ReLU, we also import softmax activation function from scipy . In my experiments, I found that this is the most stable implementation, so I did not want to run into \"not invented here\" problem. softmax is defined as follows: $$\\mathcal{S}(x)=\\left[\\frac{\\exp(x_i)}{\\sum\\limits_{k}\\exp(x_k)}\\right], i=1..n,~x=[x_1, ... , x_n]$$ Softmax accepts a vector of network's output and converts it to a vector of probability values. For this function we need to use one-hot encoding. python from scipy.special import softmax as s def softmax(x: Tensor) -> Tensor: \"\"\"Calculate softmax using scipy.\"\"\" return s(x, axis=1) Loss Functions We implement two loss functions here. We will implement Mean Squared Error Loss class and a Cross Entropy Loss class. Mean Squared Error Loss This is a very straight-forward loss function, it takes the output of the last layer of the neural network \\(\\hat{y}\\) and ccomputes: $$\\mathcal{L}(y, \\hat{y}) = \\frac{1}{2m}||y-\\hat{y}||&#94;2,$$ where \\(m\\) is the size of \\(y\\) and $$\\hat{y}$$ and $$\\vert\\vert...\\vert\\vert$$ represents the vector norm (sum of squared component-wise differences). The gradient of this function for backpropagation is computed as $$\\nabla{\\mathcal{L}}=\\frac{1}{m}(y-\\hat{y})$$ Cross Entropy Loss Cross entropy loss function is defined as follows. Let \\(\\hat{y}\\) be the so-called \\({logits}\\) , the outputs of the neural network. Then, we use softmax to calculate the probabilities \\(p=\\mathcal{S}\\left(\\hat{y}\\right)\\) . The cross entropy is $$\\mathcal{L}(y, \\hat{y}) = -\\sum\\limits_{i} y_i \\log p_i,$$ where \\(y_i\\) is the true label vector. To evaluate the gradient, consider the following argument $$\\nabla{\\mathcal{L}}(y, \\hat{y}) = -\\sum\\limits_i \\frac{\\partial \\left(y_i \\log [\\mathcal{S}(x)]_i\\right)}{\\partial x_j}$$ where \\(x\\) is the network's output. Continuing this, we obtain $$\\nabla{\\mathcal{L}}(y, \\hat{y}) = -\\sum\\limits_i y_i \\frac{1}{p_i}\\frac{\\partial \\mathcal{S}(x)_i}{\\partial x_j}$$ To find the derivative of softmax, consider $$\\frac{\\partial \\mathcal{S}_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j}\\left(\\frac{\\exp(x_i)}{\\sum\\limits_{k}\\exp(x_k)}\\right) = \\frac{\\frac{\\partial\\exp(x_i)}{\\partial x_j} \\sum\\limits_{k}\\exp(x_k) - \\exp(x_i) \\sum\\limits_{k}\\frac{\\partial\\exp(x_k)}{\\partial x_j}}{\\left(\\sum\\limits_{k}\\exp(x_k)\\right)&#94;2},$$ $$\\frac{\\partial \\mathcal{S}_i}{\\partial x_j} = \\frac{\\exp(x_i)\\delta_{ij} \\sum\\limits_{k}\\exp(x_k) - \\exp(x_i)\\exp(x_j)}{\\left(\\sum\\limits_{k}\\exp(x_k)\\right)&#94;2},$$ $$\\frac{\\partial \\mathcal{S}_i}{\\partial x_j} = \\frac{\\exp(x_i)}{ \\sum\\limits_{k}\\exp(x_k)}\\delta_{ij} - \\frac{\\exp(x_i)}{\\sum\\limits_{k}\\exp(x_k)}\\frac{\\exp(x_j)}{\\sum\\limits_{k}\\exp(x_k)} = \\mathcal{S}_i\\delta_{ij} - \\mathcal{S}_i \\mathcal{S}_j.$$ We use \\(\\delta_{ij}\\) to represent Kronecker delta-symbol (essentially, identity matrix). Finally, we can interchange the notation \\(\\mathcal{S}_i\\) for \\(p_i\\) , since both represent the \\(i\\) th component of the softmax output (the probability) $$\\nabla{\\mathcal{L}}(y, \\hat{y}) = -\\sum\\limits_i y_i \\frac{1}{p_i}p_i(\\delta_{ij}-p_j) = - y_j + p_j\\sum\\limits_i y_i.$$ Recall, that the vector \\(y\\) is one-hot encoded, therefore, the sum of its components \\(\\sum\\limits_i y_i=1\\) . Hence, we obtain $$\\nabla{\\mathcal{L}}(y, \\hat{y}) =p_j - y_j.$$ ```python class Loss: \"\"\"Placeholder class for losses.\"\"\" def __init__(self): \"\"\"Initialize the class with 0 gradient.\"\"\" self.grad = 0. def grad_fn(self, pred: Tensor, true: Tensor) -> Tensor: \"\"\"Create placeholder for the gradient funtion.\"\"\" pass def loss_fn(self, pred: Tensor, true: Tensor) -> Tensor: \"\"\"Create placeholder for the loss funtion.\"\"\" pass def __call__(self, pred: Tensor, true: Tensor): \"\"\"Calculate gradient and loss on call.\"\"\" self.grad = self.grad_fn(pred, true) return self.loss_fn(pred, true) class MSE(Loss): \"\"\"Mean squared error loss.\"\"\" def __init__(self): \"\"\"Initialize via superclass.\"\"\" super().__init__() def grad_fn(self, pred: Tensor, true: Tensor) -> Tensor: \"\"\"Calculate the gradient of MSE. Args: pred: Tensor of predictions (raw output), shape (batch, ) true: Tensor of true labels, shape (batch, ) \"\"\" return (pred - true)/true.shape[0] def loss_fn(self, pred: Tensor, true: Tensor) -> Tensor: \"\"\"Calculate the MSE. Args: pred: Tensor of predictions (raw output), shape (batch,) true: Tensor of true labels (raw output), shape (batch,) \"\"\" return 0.5*np.sum((pred - true)**2)/true.shape[0] def __repr__(self): \"\"\"Put pretty representation in Jupyter/IPython.\"\"\" return \"\"\"Mean Squared Error loss (pred: Tensor, true: Tensor)\"\"\" class CrossEntropyLoss(Loss): \"\"\"CrossEntropyLoss class.\"\"\" def __init__(self) -> None: \"\"\"Initialize via superclass.\"\"\" super().__init__() def loss_fn(self, logits: Tensor, true: Tensor) -> Tensor: \"\"\"Calculate loss. Args: logits: Tensor of shape (batch size, number of classes), raw output of a neural network true: Tensor of shape (batch size,), a one-hot encoded vector \"\"\" p = softmax(logits) return -np.mean(true * np.log(p)) def grad_fn(self, logits: Tensor, true: Tensor) -> Tensor: \"\"\"Calculate the gradient. Args: logits: Tensor of shape (batch size, number of classes), raw output of a neural network true: Tensor of shape (batch size, number of classes), a one-hot encoded vector \"\"\" self.probabilities = softmax(logits) return self.probabilities - true ``` Building the Network Here we describe the main class for our neural network. The main principle is simple, we pass a list of layers and initialize a class Network with two methods: forward and backward . The forward method performs the forward pass, that is, sends the input data through each layer. The backward method calls backward from each layer in the opposite direction (starting with the last layer). It uses the gradient of the lost function as its input. ```python from typing import List, Union Layer = Union[Linear, ReLU, Sigmoid] class Network: \"\"\"Basic Neural Network Class.\"\"\" def __init__(self, layers: List[Layer]): \"\"\"Initialize the Netowrk with a list of layers.\"\"\" self.layers = layers[:] def forward(self, x: Tensor): \"\"\"Run the forward pass.\"\"\" for l in self.layers: x = l(x) return x def backward(self, grad: Tensor): \"\"\"Run the backward pass.\"\"\" for l in self.layers[::-1]: grad = l.backward(grad) return grad def __call__(self, x: Tensor): \"\"\"Run the forward pass on __call__.\"\"\" return self.forward(x) def __repr__(self) -> str: \"\"\"Print the representation for the network.\"\"\" return \"\\n\".join(l.__repr__() for l in self.layers) ``` Optimizers Our main optimizer here is going to be Stochastic Gradient Descent. After we computed the backpropagation, for every layer in the network, we are going to update the weights. If the gradient is \\(\\Delta w\\) then the update rule is $$w = w - \\eta \\Delta w - 2*\\alpha w,$$ where \\(\\eta\\) is the learning rate and \\(\\alpha\\) is the \\(L&#94;2\\) regularization parameter. The code is presented below. ```python class SGD: \"\"\"Stochastic Gradient Descent class.\"\"\" def __init__(self, lr: float, l2: float = 0.): \"\"\"Initialize with learning rate and l2-regularization parameter.\"\"\" self.lr = lr self.l2 = l2 def step(self, net: Network): \"\"\"Perform optimization step.\"\"\" for l in net.layers: if hasattr(l, 'dydw'): l.W = l.W - self.lr*l.dydw - 2 * self.l2 * l.W if hasattr(l, 'dydb'): l.b = l.b - self.lr*l.dydb - 2 * self.l2 * l.b ``` Training MNIST in Batches using MSE In the code below, we create a training/validation loop. Each important point is commented. ```python from tqdm import auto import numpy as np import pandas as pd from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def to_one_hot(vector: Tensor) -> Tensor: \"\"\"Create one hot encoding of a vector.\"\"\" oh = np.zeros((vector.shape[0], vector.max()+1)) oh[np.arange(vector.shape[0]), vector] = 1 return oh Load training data train = pd.read_csv('mnist_train.csv', header=None).values[:, 1:] train_label = pd.read_csv( 'mnist_train.csv', header=None).values[:, 0] Create the basic network net = Network(layers=[ Linear(784, 128), ReLU(), Linear(128, 10), ]) Initialize loss class loss = MSE() Initialize the optimizer, learning rate is 0.0001 optim = SGD(1e-4) permform the train/val split x_train, x_val, y_train, y_val = train_test_split( train.astype(np.float32) / 255, train_label.astype(np.int32), test_size=0.2, random_state=42) # to_one_hot Convert labels to one-hot encodings y_train = to_one_hot(y_train) y_val = to_one_hot(y_val) batch size batch_size = 100 progress bar may not be visible in PDF mode, but it works in notebook or terminal mode we set it to 100 epochs here progress_bar = auto.tqdm(range(100)) for epoch in progress_bar: # offset to iterate through batches offset = 0 # initialize errors for validation and training val_err = 0 err = 0 while (offset+batch_size <= len(x_train)): # while we can move through batches, extract them data = x_train[offset:offset+batch_size, :] label = y_train[offset:offset+batch_size] # make prediction pred = net(data) # calculate loss (and average error) err += loss(pred, label)/(len(x_train)/batch_size) # begin backprop g = net.backward(loss.grad) # perform SGD step optim.step(net) # move to next batch offset += batch_size # reset offset for validation offset = 0 while (offset+batch_size <= len(x_val)): # get validation data while we are not at the end val_data = x_val[offset:offset+batch_size, :] val_label = y_val[offset:offset+batch_size] # make prediction pred = net(val_data) # get loss and error val_err += loss(pred, val_label)/(len(x_val)/batch_size) # move offset to next batch offset += batch_size if (epoch) % 2 == 0: # update progress bar info progress_bar.set_postfix({\"Mean_loss_train\": err, \"Mean_loss_val\": val_err}) ``` ```python Load test data and convert to one-hot test = pd.read_csv('mnist_test.csv', header=None).values[:, 1:] test_label = pd.read_csv('mnist_test.csv', header=None).values[:, 0] test_label = to_one_hot(test_label) place offset and initialize error to 0 offset = 0 test_err = 0. while (offset+batch_size <= len(test)): # get data batch data = test[offset:offset+batch_size, :] label = test_label[offset:offset+batch_size] # make prediction pred = net(data) # get error test_err += loss(pred, label)/(len(test)/batch_size) offset += batch_size print(f\"Test Error is {test_err:.2f} ...\") ``` Test Error is 2643.26 ... ```python from sklearn.metrics import confusion_matrix import seaborn as sns import matplotlib.pyplot as plt from IPython.display import set_matplotlib_formats set_matplotlib_formats('retina') plt.style.use('ggplot') %matplotlib inline y_true = test_label.argmax(1) y_pred = net(test).argmax(1) ax = plt.figure(figsize=(15, 7)) ax = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=\".3f\") ax.set_xlabel(\"True\") ax.set_ylabel(\"Predicted\") ``` Text(114.0, 0.5, 'Predicted') We can see from the confusion matrix above that the model performs poorly if the training is based on MSE. Let us try a different loss function: Cross Entropy loss. Cross Entropy Training ```python \"\"\"Training example for a simple network with MNIST Dataset.\"\"\" from tqdm import auto import numpy as np import pandas as pd from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from datatype import Tensor def to_one_hot(vector: Tensor) -> Tensor: \"\"\"Create one hot encoding of a vector.\"\"\" oh = np.zeros((vector.shape[0], vector.max()+1)) oh[np.arange(vector.shape[0]), vector] = 1 return oh Load training data train = pd.read_csv('mnist_train.csv', header=None).values[:, 1:] train_label = pd.read_csv( 'mnist_train.csv', header=None).values[:, 0] Create the network net = Network(layers=[ Linear(784, 128), Sigmoid(), Linear(128, 10), ]) Initialize loss class loss = CrossEntropyLoss() Initialize optimizer with regularization optim = SGD(5e-2, 0.0001) split x_train, x_val, y_train, y_val = train_test_split( train.astype(np.float32) / 255, train_label.astype(np.int32), test_size=0.2, random_state=42) # to_one_hot to one-hot y_train = to_one_hot(y_train) y_val = to_one_hot(y_val) batch_size = 100 progress_bar = auto.tqdm(range(200)) this will be used later accuracies: dict = {\"train\": [], \"val\": [], \"test\": []} acc_train: list = [] acc_val: list = [] for epoch in progress_bar: offset = 0 val_err = 0 err = 0 while (offset+batch_size <= len(x_train)): # grab the batch data = x_train[offset:offset+batch_size, :] label = y_train[offset:offset+batch_size, :] # I try to avoid a runtime warning (only happens in notebook, not sure why) try: pred = net(data) except RuntimeWarning: print(f\"Runtime warning on {offset}\") # get loss err += loss(pred, label)/(len(x_train)/batch_size) # backprop g = net.backward(loss.grad) # update weights optim.step(net) # next batch index offset += batch_size # keep scores acc_train.append(accuracy_score( label.argmax(axis=1), pred.argmax(axis=1) )) offset = 0 while (offset+batch_size <= len(x_val)): # get validation data val_data = x_val[offset:offset+batch_size, :] val_label = y_val[offset:offset+batch_size] # predict pred = net(val_data) # get loss val_err += loss(pred, val_label)/(len(x_val)/batch_size) # next batch index offset += batch_size # keep scores acc_val.append(accuracy_score( val_label.argmax(axis=1), pred.argmax(axis=1) )) if (epoch) % 2 == 0: # update progress bar progress_bar.set_postfix({\"loss_train\": err, \"loss_val\": val_err, \"acc_val\": np.mean(acc_val)}) # keep scores for visualization accuracies['train'].append(np.mean(acc_train)) accuracies['val'].append(np.mean(acc_val)) acc_train = [] acc_val = [] Load test data and convert to one-hot test = pd.read_csv('mnist_test.csv', header=None).values[:, 1:] test_label = to_one_hot(pd.read_csv( 'mnist_test.csv', header=None).values[:, 0]) offset = 0 test_err = 0. while (offset+batch_size <= len(test)): # get batch data = test[offset:offset+batch_size, :] label = test_label[offset:offset+batch_size] # predict pred = net(data) # get loss test_err += loss(pred, label)/(len(test)/batch_size) # offset offset += batch_size # get scores accuracies['test'].append(accuracy_score( label.argmax(axis=1), pred.argmax(axis=1) )) print(f\"Average Test Accuracy: {np.mean(accuracies['test']):.2f}\") ``` Average Test Accuracy: 0.95 Let us plot the evolution of accuracies during testing and confusion matrix. For a higher performing model we expect to see the confusion matrix consolidate results on the diagonal: python fig = plt.figure(figsize=(15, 6)) _ = plt.plot(accuracies['train'], label=\"Training score\") _ = plt.plot(accuracies['val'], label=\"Validation score\") _ = plt.xlabel(\"Epoch\") _ = plt.ylabel(\"Accuracy\") _ = plt.title(\"Accuracy per epoch\") _ = plt.legend() ```python y_true = test_label.argmax(1) y_pred = net(test).argmax(1) _ = plt.figure(figsize=(15, 7)) ax = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt=\".3f\") =ax.set_xlabel(\"True\") =ax.set_ylabel(\"Predicted\") ``` Conclusion We implemented a neural network class that supports several activation functions. We followed here a design pattern based on PyTorch deep learning package. We implemented linear (fully-connected) layer, ReLU and Sigmoid layer. Each layer includes a backpropagation function backward that sends the gradient from the output back to input. As a result we were able to use a Cross Entropy Loss function to train a handwritten digit classifier with 95% accuracy on the test set. Notice that on the graph we observe a pattern of periodically dropping accuracy. I assume this is due to internal structure of the loss landscape: we repeatedly \"walk\" out of the minimum region and then \"walk\" back in during the SGD. Using the Mean Squared Error loss function did not yield a productive result here, however, while developing this library, I observed that if I train on a small sample of data (i.e. 50 items or less), the model was able to learn the underlying data representations very well and was able to over fit. This is one the tests usually performed on new architectures in order to check if the model can learn at all. This, unfortunately, did not scale in case of MSE but it did for Cross Entropy Loss. The overall structure of the project is posted on my github page","tags":"Posts","url":"https://iliailmer.github.io/2020/06/numpy-learn.html","loc":"https://iliailmer.github.io/2020/06/numpy-learn.html"},{"title":"Three Ways to Deal With Imbalance","text":"In this post, I put together an interesting example of what to do with imbalanced datasets and why precision and recall matter. Introduction The following is part of a Machine learning assignment I had to do while at CUNY. This particular example illustrates quite well the importance of understanding various measures of model quality such as accuracy, precision, recall, etc. The idea is to predict whether a client will have a credit card default given some simple data. An insult to injury is a heavily imbalanced dataset: 97% of examples are \"Yes\"-labeled, making it difficult to train a good classifier. We will dive into the solution, but first, some imports: python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import numpy as np import gc from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, precision_score, recall_score from sklearn.linear_model import LogisticRegressionCV, LogisticRegression plt.style.use('fivethirtyeight') A plotting helper: py def plot_cm(y_pred, y_true): \"\"\"Plot confusion matrix based on true vs. predicted values.\"\"\" cm = confusion_matrix(y_true=y_true, y_pred=y_pred) prec = precision_score(y_true=y_true, y_pred=y_pred) rec = recall_score(y_true=y_true, y_pred=y_pred) sns.heatmap(cm, annot=True) plt.title(f\"Precision: {prec:.2f}, Recall: {rec:.2f}\") Finally, reading the data : ```python df = pd.read_excel(\"Default.xlsx\", header=0).iloc[:, 1:] convert strings to binary int32 0,1 values df['default'] = df['default'].apply(lambda x: 0 if x == 'No' else 1).astype( np.int32) df['student'] = df['student'].apply(lambda x: 0 if x == 'No' else 1).astype( np.int32) X = df.drop(['default'], axis=1) y = df['default'] ``` To save memory: convert applicable columns from float64(double) to float32(float). This may help with much bigger datasets, this one is relatively small, though. Applicable here means max value of a column falls within the limits of the float32 range: ```py for column in df.columns: if df[column].values.max() < np.finfo(np.float32).max: if df[column].values.min() > np.finfo(np.float32).min: df[column] = df[column].values.astype(np.float32) garbage collector gc.collect() ``` Let's look at the distribution of the target value: python ax = sns.countplot(data=df, x='default') The histogram above verifies the problem: the data is highly imbalanced. To train a linear regression classifier, we will try to remedy the issue with three approaches: We sample a training, validation, and testing subsets in a stratified manner, that is, preserving the ratio between \"0\" and \"1\" labels. We will oversample the less represented class: \"1\", or \"Yes\"-labeled default examples. We will undersample the overrepresented class: \"0\". Stratified Sampling To sample in a way that the imbalance is preserved (stratified) we will pass an argument stratify to the train_test_split function from sklearn . Specifically, we will pass the column of labels df['default'] so that the function determines the exact distribution. ```python df_train, df_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=df['default'], train_size=0.6) df_val, df_test, y_val, y_test = train_test_split(df_test, y_test, random_state=42, stratify=y_test, train_size=0.5) ``` The plot below will illustrate the distribution of labels in the sampled data. python fig, ax = plt.subplots(ncols=3, figsize=(12, 4)) fig.suptitle(\"Left to right: train, val, test label counts.\") for i, data in zip(range(3), (y_train, y_val, y_test)): sns.countplot(x=data, ax=ax[i]) ```python print(f'Train data to original dataset: {len(df_train)/len(X) * 100}%') print(f'Validation data to original dataset: {len(df_val)/len(X) * 100}%') print(f'Test data to original dataset: {len(df_test)/len(X) * 100}%') print(f'\\nNegative labels in original data: {len(X[y==0])/len(X) * 100 :.3}%') print(f'Negative labels in train data: {(len(df_train[y_train==0])/len(df_train) * 100) :.3}%') print(f'Negative labels in validation data: {(len(df_val[y_val==0])/len(df_val) * 100) :.3}%') print(f'Negative labels in test data: {(len(df_test[y_test==0])/len(df_test) * 100) :.3}%') ``` Train data to original dataset: 60.0% Validation data to original dataset: 20.0% Test data to original dataset: 20.0% Negative labels in original data: 96.7% Negative labels in train data: 96.7% Negative labels in validation data: 96.7% Negative labels in test data: 96.7% Oversampling For oversampling we will use a package called imbalanced-learn available from PyPI . It has built-in classes for various over- and under-sampling methods. We will use basic RandomOverSampler and RandomUnderSampler classes. The package is built with similarities to other sklearn conventions, so calling fit methods will give us the necessary results. ```python from imblearn.over_sampling import RandomOverSampler ros = RandomOverSampler(random_state=42) # initialize df_train_over, y_train_over = ros.fit_sample( df_train, y=y_train) # resample training data ax = sns.countplot(x=y_train_over) # plot value counts for the labels t = plt.suptitle(\"Oversampled value counts.\") ``` Underspamling ```python from imblearn.under_sampling import RandomUnderSampler rus = RandomUnderSampler(random_state=42, ) # initialize df_train_under, y_train_under = rus.fit_sample(df_train, y=y_train) # resample ax = sns.countplot(x=y_train_under) # plot t = plt.suptitle(\"Undersampled value counts.\") ``` Train naively without stratification Let us first train logistic regression model on a similarly split data without stratification. ```python df_train_naive, df_test_naive, y_train_naive, y_test_naive = train_test_split( X, y, random_state=42, train_size=0.6) df_val_naive, df_test_naive, y_val_naive, y_test_naive = train_test_split( df_test_naive, y_test_naive, random_state=42, train_size=0.6) ``` ```python logreg = LogisticRegression(penalty='none', max_iter=1e4, random_state=42) logreg.fit(df_train_naive, y_train_naive) y_pred = logreg.predict(df_val_naive) plot_cm(y_val_naive, y_pred) ``` We observe zero precision and zero recall scores and the model overfits on the '0' class, which is expected and is bad. Train on Stratified Below we train on the stratified data. We observe a better recall and precision scores and the predicted result on validation data follows the distribution of labels as in the training data (note that this heuristic is absolutely non-strict and does not necessarily indicate whether a model is good or bad). ```python logreg = LogisticRegression(penalty='none', max_iter=1e4, random_state=42) logreg.fit(df_train, y_train) y_pred = logreg.predict(df_val) plot_cm(y_val, y_pred) ``` python ax = plt.subplot() ax = sns.countplot(y_pred, ax=ax) Train on Oversampled When training on Oversampled data we get a very high precision score but lose the recall score. ```python logreg = LogisticRegression(penalty='none', max_iter=1e4, random_state=42) logreg.fit(df_train_over, y_train_over) y_pred = logreg.predict(df_val) plot_cm(y_val, y_pred) ``` ```python y_pred = logreg.predict(df_test) plot_cm(y_test, y_pred) ``` Train on Undersampled A similar behavior occurs in the undersampled case, we see increase in Precision compared to naive and stratified cases, but both precision and recall are a bit lower that oversampled case. ```python logreg = LogisticRegression(penalty='none', max_iter=1e4, random_state=42) logreg.fit(df_train_under, y_train_under) y_pred = logreg.predict(df_val) plot_cm(y_val, y_pred) ``` Conclusion In this assignment, we considered a problem of default prediction for a credit card holder. We omitted most of preliminary data analysis and instead focused on basic model building for a highly imbalanced dataset. The proportion of negative labels is 97% out of all given data. Let us briefly outline meaning behind precision and recall scores. Precision score shows the ratio of true positive predictions over all positive predictions regardless if they were true or false. If the number of true positive predictions is negligible compared to false positives then precision is low. Recall score , on the other hand, shows the ratio of true positives versus the sum of true positive and false negative predictions. That is, if the prediction's number of true positive cases is trumped by the number of false negative cases, the recall score will be higher. Card default is essentially the inability to pay off the card's balance. In this case, we are not willing to accept a false negative prediction: if we forecast that the default does not happen and in reality it does then we (the bank) are in the losing position. On the other hand, the false positive case does not affect us because in the worst case the cardholder pays off their debt and the default does not happen. Since we want to be as effective as possible in our prediction, we must recommend a model with a higher recall score, which in this case is a stratified logistic recall score model. ```python logreg = LogisticRegression(penalty='none', max_iter=1e4, random_state=42) logreg.fit(df_train, y_train) y_pred = logreg.predict(df_val) plot_cm(y_val, y_pred) ``` ```python y_pred = logreg.predict(df_test) plot_cm(y_test, y_pred) ```","tags":"Posts","url":"https://iliailmer.github.io/2020/03/precision-recall.html","loc":"https://iliailmer.github.io/2020/03/precision-recall.html"},{"title":"Linear Regression as the Simplest Classifier","text":"In this post I wanted to describe a simple application of a linear least squares method to a problem of data classification. It is a naive approach and is unlikely to beat more sophisticated techniques like Logistic Regression , for instance. Imports Some imports we are going to need for this piece. ```python import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt %matplotlib inline plt.style.use(\"ggplot\") ``` Prepare the data Data can be found here . The link also has information about the textbook with excellent theoretical background on much of Machine Learning (and Computational Statistics). The data is a collection of hand-written digits. Each row of the data represents pixels of the image. For simplicity, we discard all labels except 2 and 3 making the problem a binary classification one (since only two labels are involved). We load files with space character as a separator and no header python train = pd.read_csv('zip.train', sep=' ', header=None).drop(257, axis=1) test = pd.read_csv('zip.test', sep=' ', header=None) Select only required labels: (by default labels are read as float64 but we won't worry about that) python train = train.loc[train[0].isin([2.0, 3.0])].reset_index(drop=True) test = test.loc[test[0].isin([2.0, 3.0])].reset_index(drop=True) ```python convert types to int32 and replace labels with 0 and 1 train[0] = train[0].astype(np.int32).map({2: 0, 3: 1}) test[0] = test[0].astype(np.int32).map({2: 0, 3: 1}) X, y = train.iloc[:, 1:], train.iloc[:, 0] X_test, y_test = test.iloc[:, 1:], test.iloc[:, 0] ``` python X.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1389 entries, 0 to 1388 Columns: 256 entries, 1 to 256 dtypes: float64(256) memory usage: 2.7 MB python y.describe() count 1389.000000 mean 0.473722 std 0.499489 min 0.000000 25% 0.000000 50% 0.000000 75% 1.000000 max 1.000000 Name: 0, dtype: float64 python X_test.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 364 entries, 0 to 363 Columns: 256 entries, 1 to 256 dtypes: float64(256) memory usage: 728.1 KB python y_test.describe() count 364.000000 mean 0.456044 std 0.498750 min 0.000000 25% 0.000000 50% 0.000000 75% 1.000000 max 1.000000 Name: 0, dtype: float64 Some helper functions ```python def show_sample(data: pd.DataFrame): \"\"\"Show 4 images from the dataset (random).\"\"\" sample = data.sample(4) fig, ax = plt.subplots(2, 2, figsize=(10, 8)) for i in range(2): for j in range(2): ax[i, j].imshow(sample.iloc[2 * i + j, 1:].values.reshape(16, 16)) ax[i, j].axis('off') ax[i, j].set_title(f'Label: {sample.iloc[2*i+j, 0]}') plt.show() def raw2label(y_pred, threshold=0): \"\"\"Convert raw label into a int label with a threshold.\"\"\" return np.int32(y_pred > threshold) ``` python show_sample(test) Linear Regression fitting We will approach the solution in the following way. Firstly, we will train a linear regression model on the available data (256 pixel intensity values). Note that the problem is high-dimensional and therefore it is impossible to visualize all of it at once. However, the main idea of the linear regression based classifier remains simple: we find a hyperplane in the \\(p+1=257\\) dimensional space that fits the training data perfectly: $$ \\hat{y}=\\mathbf{X}\\hat{\\beta},$$ where \\(X_i\\) is the input column vector (features) with intercept (that is \\(X_0=1\\) ): $$\\\\X_i=(1, X_{i1} ,\\dots X_{ip} )&#94;T \\in \\mathbb{R}&#94;{p+1},$$ \\(\\mathbf{X}\\) is the matrix of features $$\\\\\\mathbf{X} = [X_1&#94;T,\\dots, X_n&#94;T]\\in \\mathbb{R}&#94;{n\\times(p+1)}\\text{},$$ and \\(\\beta=[\\beta_0, \\dots, \\beta_p]\\) is the vector of parameters $$\\\\\\beta=[\\beta_0, \\dots, \\beta_p]\\in \\mathbb{R}&#94;{p+1},$$ where \\(\\hat{\\beta}\\) is given by $$ \\hat{\\beta} = (X&#94;TX)&#94;{-1}X&#94;Ty.$$ We are projecting our training set \\(\\mathbf{X}\\) orthogonally onto the hyperplane defined by \\(\\hat\\beta\\) . We can then derive a classifier from there as follows: let us set up the threshold \\(t\\) such that if for a given unseen feature vector \\(X_{test}\\) the value \\(y_{test}=X_{test}&#94;T\\hat\\beta=\\hat\\beta_0+\\sum\\limits_{i=1}&#94;p (X_{test})_i\\hat\\beta_i\\) is greater than \\(t\\) , we assign label 1, otherwise 0. python linreg = LinearRegression() linreg.fit(X, y) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Train set predictions python y_true = y.copy().values y_pred = linreg.predict(X) We select the threshold range between the minimum and maximum value of the predicted \\(y\\) with a step 0.1 and then check the accuracy for each threshold. ```python thresh_range = np.arange(y_pred.min(), y_pred.max(), 0.1) train_acc_scores = [] for t in thresh_range: train_acc_scores.append( accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true)) ``` ```python fig, ax = plt.subplots(ncols=2, figsize=(24, 4)) ax[0].plot(thresh_range, train_acc_scores, '-o') ax[0].set_title(\"Accuracy on Train Set\") ax[0].set_xlabel(\"Threshold\") ax[0].set_ylabel(\"Accuracy score\") ax[0].set_ylim([0.4, 1.05]) max_score = max(train_acc_scores) max_loc = np.argmax(train_acc_scores) ax[0].annotate( f'Max Accuracy: {max_score:.3f}', xy=(thresh_range[max_loc], max_score), xytext=(thresh_range[max_loc], max_score + 0.025), arrowprops=dict(facecolor='black', shrink=0.01), ) ax[1].plot(thresh_range, 1 - np.array(train_acc_scores), '-o') ax[1].set_title(\"Error Train Set\") ax[1].set_xlabel(\"Threshold\") ax[1].set_ylabel(\"Error\") min_score = min(1 - np.array(train_acc_scores)) min_loc = np.argmin(1 - np.array(train_acc_scores)) ax[1].annotate( f'Min Error: {min_score:.3f}', xy=(thresh_range[min_loc], min_score), xytext=(thresh_range[min_loc], min_score + 0.025), arrowprops=dict(facecolor='black', shrink=0.01), ) fig.text( 0.5, -0.1, 'As the threshold increases we observe simultaneous increase in accuracy.\\n'+\\ 'As we cross the maximum accuracy, we essentially shift the hyperplane away from the data and thus see the decrease.', ha='center', fontsize=16); ``` Test set predictions We can do similar analysis on the testing set. python y_true = y_test.copy().values y_pred = linreg.predict(X_test) test_acc_scores = [] for t in thresh_range: test_acc_scores.append( accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true)) ```python fig, ax = plt.subplots(ncols=2, figsize=(24, 6)) ax[0].plot(thresh_range, test_acc_scores, '-o') ax[0].set_title(\"Accuracy on Test Set\") ax[0].set_ylim([0.4, 1.05]) ax[0].set_xticks(thresh_range) ax[0].set_xlabel(\"Threshold\") ax[0].set_ylabel(\"Accuracy score\") max_score = max(test_acc_scores) max_loc = np.argmax(test_acc_scores) ax[0].annotate( f'Max Accuracy: {max_score:.3f}', xy=(thresh_range[max_loc], max_score), xytext=(thresh_range[max_loc] + 0.05, max_score + 0.05), arrowprops=dict(facecolor='black', shrink=0.01), ) ax[1].plot(thresh_range, 1 - np.array(test_acc_scores), '-o') ax[1].set_title(\"Error Test Set\") ax[1].set_xlabel(\"Threshold\") ax[1].set_ylabel(\"Error\") ax[1].set_ylim([0, 0.55]) min_score = min(1 - np.array(test_acc_scores)) min_loc = np.argmin(1 - np.array(test_acc_scores)) ax[1].annotate( f'Min Error: {min_score:.3f}', xy=(thresh_range[min_loc], min_score), xytext=(thresh_range[min_loc] - 0.15, min_score - 0.03), arrowprops=dict(facecolor='black', shrink=0.01), ) fig.text( 0.5, -0.1, 'We observe a similar behaviour as the threshold increases.\\n'+\\ 'Notice that we use the same threshold as in the test set. '+\\ 'The graphs converge around random guesses.', ha='center', fontsize=16); ``` python percentage = np.unique(y_test, return_counts=True)[1]/len(y_test) # proportion of each label in the training set print(f'Minimal accuracy on the left: {min(test_acc_scores)}, proportion of \"0\" labels in the data: {percentage[0]}') print(f'Minimal accuracy on the right: {test_acc_scores[-1]}, proportion of \"1\" labels in the data: {percentage[1]}') Minimal accuracy on the left: 0.46153846153846156, proportion of \"0\" labels in the data: 0.5439560439560439 Minimal accuracy on the right: 0.5467032967032966, proportion of \"1\" labels in the data: 0.45604395604395603 From the information above we learn that the model start to approximately randomly guess once the thresholds are too big or too small. On the left in is closer to guessing ones, on the right - zeros. python print(f\"Labels from leftmost threshold:\\n{raw2label(y_pred, threshold=thresh_range[0])}\") print(f\"\\n\\nLabels from rightmost threshold:\\n{raw2label(y_pred, threshold=thresh_range[-1])}\") Labels from leftmost threshold: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] Labels from rightmost threshold: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] Let us use different thresholds for the testing set: python y_true = y_test.copy().values y_pred = linreg.predict(X_test) thresh_range = np.arange(y_pred.min(), y_pred.max(), 0.1) test_acc_scores = [] for t in thresh_range: test_acc_scores.append( accuracy_score(y_pred=raw2label(y_pred, t), y_true=y_true)) ```python fig, ax = plt.subplots(ncols=2, figsize=(24, 6)) ax[0].plot(thresh_range, test_acc_scores, '-o') ax[0].set_title(\"Accuracy on Test Set\") ax[0].set_ylim([0.4, 1.05]) ax[0].set_xticks(thresh_range) ax[0].set_xlabel(\"Threshold\") ax[0].set_ylabel(\"Accuracy score\") max_score = max(test_acc_scores) max_loc = np.argmax(test_acc_scores) ax[0].annotate( f'Max Accuracy: {max_score:.3f}', xy=(thresh_range[max_loc], max_score), xytext=(thresh_range[max_loc] + 0.05, max_score + 0.05), arrowprops=dict(facecolor='black', shrink=0.01), ) ax[1].plot(thresh_range, 1 - np.array(test_acc_scores), '-o') ax[1].set_title(\"Error Test Set\") ax[1].set_xlabel(\"Threshold\") ax[1].set_ylabel(\"Error\") ax[1].set_ylim([0, 0.55]) min_score = min(1 - np.array(test_acc_scores)) min_loc = np.argmin(1 - np.array(test_acc_scores)) ax[1].annotate( f'Min Error: {min_score:.3f}', xy=(thresh_range[min_loc], min_score), xytext=(thresh_range[min_loc] - 0.15, min_score - 0.03), arrowprops=dict(facecolor='black', shrink=0.01), ) fig.text( 0.5, -0.1, 'We observe a similar behaviour as the threshold increases.\\n'+\\ 'Notice that we now see convergence to a random guess clearer. '+\\ 'This is due to the threshold being intrinsic to the test set.', ha='center', fontsize=16); ``` ```python percentage = np.unique(y_test, return_counts=True)[1] / len( y_test) # proportion of each label in the training set print( f'Minimal accuracy on the left: {min(test_acc_scores)},' + f' proportion of \"0\" labels in the data: {percentage[0]}' ) print( f\"Labels from leftmost threshold:\\n{raw2label(y_pred, threshold=thresh_range[0])}\" ) print( f'\\n\\nMinimal accuracy on the right: {test_acc_scores[-1]},' + f' proportion of \"1\" labels in the data: {percentage[1]}' ) print( f\"Labels from rightmost threshold: \\n{raw2label(y_pred, threshold=thresh_range[-1])}\" ) ``` Minimal accuracy on the left: 0.45879120879120877, proportion of \"0\" labels in the data: 0.5439560439560439 Labels from leftmost threshold: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] Minimal accuracy on the right: 0.5412087912087912, proportion of \"1\" labels in the data: 0.45604395604395603 Labels from rightmost threshold: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] K-Nearest Neighbors For this method, we consider different values of neighbors in each case. We begin with 1 neighbor per training point, which we expect to give a high accuracy since each point will roughly be each own neighborhood. As the number of neighbors increases we will see a slight decrease in accuracy. ```python train_err_scores = [] test_err_scores = [] for k in [1, 3, 5, 7, 15]: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X, y) y_true = y.copy().values y_pred = knn.predict(X) train_err_scores.append(1 - accuracy_score(y_pred=y_pred, y_true=y_true)) y_true = y_test.copy().values y_pred = knn.predict(X_test) test_err_scores.append(1 - accuracy_score(y_pred=y_pred, y_true=y_true)) ``` ```python fig, ax = plt.subplots(ncols=2, figsize=(24, 6)) ax[1].plot([1, 3, 5, 7, 15], train_err_scores, '-o') ax[1].set_title(\"Error on Train Set\") ax[1].set_xlabel(\"Neighbors\") ax[1].set_ylabel(\"Error score\") ax[0].plot([1, 3, 5, 7, 15], test_err_scores, '-o') ax[0].set_title(\"Error on Test Set\") ax[0].set_xlabel(\"Neighbors\") ax[0].set_ylabel(\"Error score\") fig.text( 0.5, -0.1, 'As the number of neighbors increases, so does the error score of the model on the training data.', ha='center', fontsize=16); ``` Conclusion We applied two different classification methods for the data. Method 1 was based on the linear regression using least squares method. The second method was based on 5 different Nearest neighbor classifiers. Each approach showed different results specific to assumptions underlying each model. Linear model assumes that the relationship between the label and pixel intensities is a high-dimensional linear function. Using various threshold we were able to show the change in hyperplane location relative to test and train data and how the accuracy scores (error scores) reflect this change. For the k-NN model, we observed a similar behavior: going from few neighbors to more, we saw that the model is prone to the larger error. This is to be expected but not to say that 1 neighbor is necessarily better than 15 neighbors, since 1 neighbor case is overfitting the data. The data is very high-dimensional so it is harder to evaluate how many neighbors we should aim for here.","tags":"Posts","url":"https://iliailmer.github.io/2020/02/data-explore.html","loc":"https://iliailmer.github.io/2020/02/data-explore.html"},{"title":"How to write a decent training loop with enough flexibility.","text":"In this post, I briefly describe my experience in setting up training with PyTorch. Introduction PyTorch is an extremely useful and convenient framework for deep learning. When it comes to working on a deep learning project, I am more comfortable with PyTorch rather than TensorFlow. In this quick post, I would like to show how one can go about building a custom training loop, something that I struggled when I was getting started. It is a useful skill to be able to build the training loop on your own because that can help you understand better what happens under the hood of a deep learning package that abstracts a lot of nuts and bolts away from the end-user. The Overview of Training When one trains a network, we need to follow a certain paradigm. First, set the model into training mode. Second, start iterating through the training set. For every batch we must: compute the output of the network compute the loss get gradients start descending using the optimizer This last step we acknowledge that the method for optimization of our model is based on a gradient descent. It can be eqither Adam, SGD, or any other (RAdam is the brand new one which seems to beat state of the art). In code, we can put it in the form like this: ```python def train(epoch): model.train() # preparing model for training for batch in training_set: x, y = batch # unpack the batch # the step below is necessary so that we update the gradient only pertinent to the current batch optimizer.zero_grad() # compute the output output = model(x.cuda()) # calculate the loss function loss = criterion(output, y.cuda()) # calculate the gradient using backpropagation loss.backward() # take a step with the optimizer optimizer.step() ``` A Trick for Better Training with Lower Memory A small batch can result in a small gradient. This, in turn, leads to a problem called vanishing gradient problem : the value is so small, computer simple treats it as zero (underflow). To avoid it, a trick of accumulating gradient as you iterate through the dataset. I saw a practical implementation in this discussion . python def train_accumulate(epoch, accumulation_step = 1): model.eval() # preparing model for training for idx, batch in enumerate(training_set): x, y = batch # unpack the batch # compute the output output = model(x.cuda()) # calculate the loss function loss = criterion(output, y.cuda()) # calculate the gradient using backpropagation loss.backward() if idx%accumulation_step==0: # take a step with the optimizer once # we accumulated enough gradients optimizer.step() optimizer.zero_grad() In Closing: abstracting training loop In this post I summarized my experience in building a training loop for PyTorch. Lately, I have been using a more abstracted way of training through Catalyst . It is a great tool for higher level abstraction during training and a lot of hardwork has been done to take away the hard part of training. Nevertheless, both, I believe, are equally important: the abstract and the explicit methods. Thanks for reading!","tags":"Posts","url":"https://iliailmer.github.io/2019/06/how-to-make-training-loop.html","loc":"https://iliailmer.github.io/2019/06/how-to-make-training-loop.html"},{"title":"RiCNN and Rotation Robustness of ConvNets. A Paper Review","text":"Lately, I have been reading more papers on modern advances in deep learning in order to get a clear view of what problem I want to focus on during my PhD research. There is a lot of information to process and an incredible amount of papers are being published from all over the world. In order to keep up, I will do my best to document everything that I read in this weekly series of paper reviews. The aim is to post one review a week and go from there. The first paper I will talk about is the one by Chidester, Do, and Ma titled Rotation Equivariance and Invariance in Convolutional Neural Networks . What is the key problem? The advantage of convnets over other algorithms is in the amount of different features such network extracts from a given input. Convnets are a very powerful and form a very ubiquitous family of algorithms with various applications in industry. When it comes to image classification, the same image or the same object in the image could be presented in various positions: it could be shifted left or right, and even rotated. The shift, or, translation , is something the network can withstand. The problem arises when the input is rotated. If the network has never seen this rotated image, the result of classification will be wrong regardless of whether or not the net has seen the un -rotated original. This originates in the nature of the convolution: the operation of sliding the feature extracting kernel along the image ( this is an excellent depiction of convolution). Convolution in its natural habitat: a blog post. One common way of dealing with this is to rotate by a random angle images in the input. This is called augmentation and it increases the number of input data, which in turn may increase training time. Such trade-off is undesirable: why increase training time and sacrifice memory for a super-large network with hundreds of thousands of images already at hand? Side note: not to say that augmentation is a bad technique. Quite the opposite , it is a very powerful way to avoid overfitting. So here is the problem: how can the network learn image transformation without explicitly seeing the transformed image during training? Rotation Equivariance In the second section of the paper, the authors propose a multi-stage approach to dealing with rotations. Stage 1 is rotation equivariance, stage 2 is a rotation invariant layer in the network just before the fully-connected classifier block, and stage 3 is the fully-connected layer itself. Rotation Equivariance The term equivariance when applied to rotation transformation means the following: if a function acts on an input then the result is . If the input is \"corrupted\" by a transformation (rotation, shift, etc.), then equivariance with respect to implies existence of a transform such that: . The invariance property would look like this: . Proposed Solution Rotation of the Filters What the authors proposed is to connect rotation of the input with that of the feature extractor explicitly. To elaborate, the convolutional kernel is, essentially, rotated for a desired range of angles. At the same time, the image is split into conic regions, each region will have a designated rotated copy of the convolution kernel. Each rotated kernel is applied to the region. This formulation opens up a new formalism: a conic convolution. Diagram of the proposed network by Chidester, Do, and Ma . The authors prove a very interesting property of this network. Essentially, the rotation equivariance property we stated still holds, according the results, however, is equal to . (This is proven in Theorem 1 of the article.) Discrete Fourier transform (DFT) After that, the results of the conic section of the network need proceed to an additional section before the fully-connected one. It is required to preserve the rotation equivariance extracted by the conic portion before unravelling the tensors. Such encoding of the equivariance is achieved through the DFT procedure. The authors note the cyclical property of the last convolutional with respect to rotation order. Applying DFT to the output of this convolution yields a representation in which the rotation is \"hard-coded\". Finally, this representation is passed into the fully connected layer. Conclusion I will not focus this review on the results of the work when applied to benchmark datasets, I believe it is only fair that one refers to the paper itself for that. Rotation equivariance and general robustness of neural networks to external peturbations is important and in applications such as medical imaging, where the data can be the same image from different angles (or with different transformations applied), a neural network must return a faulty result as the life of a patient is on the line.","tags":"Posts","url":"https://iliailmer.github.io/2019/06/paper-review.html","loc":"https://iliailmer.github.io/2019/06/paper-review.html"},{"title":"Computer Vision: Can You Teach a Machine To See?","text":"A little overview of what I talked about at CUNY CSI Science Day. During the Science Day at the CUNY College of Staten Island, I presented a gentle introduction to area of computer vision with fun examples and research results to visiting middle and high school students. It was a very interesting experience given the target audience without any experience (or, at least, presumably) in the subject. I aimed at being user friendly by showing some of the most ground-breaking online projects. My favorite part of the talk was when we played a little game of \"Do you know who this person is?\" with fake images from GANs. The reactions were priceless and the interest unmistakably there. I finished the talk with a bit of a diversion for those who might be interested in trying stuff on their own: Kaggle, FastAI, and Python (which quite a lot of them are already familiar with!) were all mentioned as something to try out.","tags":"Posts","url":"https://iliailmer.github.io/2019/03/science-day.html","loc":"https://iliailmer.github.io/2019/03/science-day.html"},{"title":"Harmonic networks: implementation of paper results","text":"I implement an interesting result from a recent paper on convolutional neural networks. Introduction In this post I will briefly discuss my implementation of a model introduced in this paper . In short, the authors suggest using predefined filters in a convolutional network based on Discrete Cosine Transform. I used PyTorch for neural network implementation, the other packages include pandas for data reading, numpy, scipy, skimage , etc. The dataset I was using is the skin cancer MNIST from Kaggle. I really enjoyed working on this project! However, it is still far from being complete and I will try to fix some errors it has in due time. Paper summary In this paper, the authors propose a modification to the common algorithm of convolutional neural networks. Classically, such networks are comprised of convolutional layers. Each layer, in turn, has a corresponding amount of kernels which are slided over the input during convolution ( see visualization here ). As the input is carried through the network, in the final, non-convolutional (fully-connected) layer the network measure the error between the true label/value and the prediction is carried backwards through back propagation algorithm. That way the weights (kernels) are adjusted to accommodate for the discrepancy in prediction and improve feature extraction, which is the primary purpose of convolutional layers. The paper, however, considers a different approach to convolutional neural networks. In suggested algorithm, the net's layers are predefined by decomposition of the input according to the Discrete Cosine Transform (the DCT). The DCT is a method for feature extraction that is based on Fourier Transform . It separates frequencies that comprise the input signal from itself and thus produce features (the frequency spectrum). The authors use convolutional approach to computing the Discrete Cosine transform, building filters that result in equivalent mathematical formulations upon convolution. Using these filters as kernels in CNN, they construct a Harmonic Network. Such network can be computationally expensive (more on that later), but it is able to show state of the art results on common image datasets such as CIFAR10. The harmonic block that replaces the usual convolutional layer consists of a linear combination of features from the DCT of the input from the previous layer. These features can optionally be batch normalized. Difficulties in implementation Let me discuss some difficulties I ran into when I was implementing the network. I will try to be as brief as I can. Finding the right kernel (filter bank) Firs of all, representing the DCT as a convolution sounds intuitively simple but turned out to be more difficult in practice. Let's look at the formula transforming the 2D N-by-N input signal \\(x\\) into 2D output signal \\(\\hat X\\) : \\(\\hat X_{u,v} = \\sum_{ii=0}&#94;{N-1}\\left[\\sum_{jj=0}&#94;{N-1} x_{ii,jj} \\cos\\left(\\frac{\\pi}{N}\\left(ii+0.5\\right) u \\right)\\right] \\cos\\left(\\frac{\\pi}{N}\\left(jj+0.5\\right) v \\right).\\) So the approach is to use a separate kernel for each combination of \\((u,~v)\\) indices. They will implicitly represent the direction in which our filter is looking at the image. So, if our sliding convolution window is \\(N\\times N\\) then we need \\(N&#94;2\\) filters, \\(N\\times N\\) each: py import torch import numpy as np PI = np.pi def fltr(u, v, N, k): # note, that I will always use N=k and N=K in where # but we can have N>K. I have not tried N<K, # it'd be pretty cool to try that as well return torch.as_tensor([[torch.cos(torch.as_tensor(v*PI/N*(ii+0.5))) * torch.cos(torch.as_tensor(u*PI/N*(jj+0.5))) for ii in range(k)] for jj in range(k)]) So, once we get the necessary filters, we need to properly collect them into the so-called filter bank . ```py import torch def get_filter_bank(input_channels, N, K): filter_bank = torch.stack([torch.stack([fltr(j, i, N, K) for i in range(K)]) for j in range(K)]) filter_bank = filter_bank.reshape([-1, 1, K, K]) filter_bank = torch.cat([filter_bank]*input_channels, dim=1) filter_bank = filter_bank.to('cuda') return filter_bank ``` Great, the filters are collected. The cool thing with PyTorch is that by default these tensors will not be updated in the backwards pass. This is because the property called requires_grad is initialized to False automatically. (I should probably add that it can always be manipulated manually, but we will not worry about that here.) Further processing of harmonic blocks One convolution with kernels is not enough. The authors propose a way of combining them linearly through \\(1\\times1\\) convolution. That way each consequent layer is a linear combination of the previous one. This convolution is affected by the backwards pass. The linear combination occurs across the result of convolution with DCT filters. If the input has 3 channels and each channel produces 9 convolution results ( \\(3\\times 3\\) filters, 9 of them) then we get the output shape after the harmonic block as (3,9,W,H) where W and H are width and height respectively. The linear combination than happens across the 9 outputs for each channel. That's it! Sending to GPU device using torch.cuda() A problem I encountered while implementing the Harmonic block of the network was that just sending the model to GPU using py model.cuda() is not enough. I had to \"manually\" send the convolution weights. Note that it's not really manually as in \"low-level\" but rather this line of code from before: py filter_bank = filter_bank.to('cuda') Not sure why the model does not send the DCT filters upon regular GPU sending but manually it all works. Selecting learning rate Another problem was the learning rate. From lessons on FastAI I learned that trying the learning rate of 0.01 is quite common (the amazing function for finding a proper learning rate consistently recommends doing so across multiple CNN models). However, in here, we get quite rapid loss increase! Update, March,2019 Experimenting with learning rate finding yielded that 0.001 learning rate with Adam optimizer works really well for my architecture, especially on an unbalanced dataset I use for my research. Image preprocessing, added March,2019 In the preprocessing stage, the images are downsized to 64 by 64 pixels using skimage library. Originals in the dataset used are 450 by 600, which can take too much of the memory. Additional preprocessing consists of hair removal described in this paper . Result Although it overfits during training, the model gives ~84% in both precision and recall which is pretty nice. Summary I really enjoyed working on the implementation. I learned a lot about how PyTorch works and how to use it when building a model from scratch. Even though this is still a work in progress for me, I will be gradually improving the implementation as much as I can, time permitting. Some things I plan to do Improve metrics on testing set Experiment with binarization of weights: to decrease model size The implementation of the model can be found here .","tags":"Posts","url":"https://iliailmer.github.io/2019/03/harmonic-network/.html","loc":"https://iliailmer.github.io/2019/03/harmonic-network/.html"},{"title":"Image Quality Measure","text":"A simple function that can be used to justify image quality and control enhancement. Introduction One difficult thing about image enhancement is to actually measure the level of image quality which is quite a subjective task. On the one hand, each individual can perceive the image quality according to their own tastes and preferences. On the other hand, our visual system is the same for all of us, no matter what tastes you have. An interesting measure that goes forward to unify the subjectivity of human taste and the visual system based perspective was introduced in this paper . Let's try it out! Code Well, the essential idea of this measure is that we focus our attention on the image in a blockwise manner. Splitting the image into blocks and then finding the maximal value of the pixel intensity per each block is fairly straightforward and simple to code. We split the \\(M\\times N\\) image, which in computer memory is represented as a matrix of pixel intensities, into submatrices each of size \\(n\\times n\\) . This results in \\(k_1\\) blocks along the vertical axis of the image and \\(k_2\\) along the horizontal one. The quality measure can be used $$\\sum_{l=1}&#94;{k_1}\\sum_{p=1}&#94;{k_2}\\frac{\\max(W_{lp})}{\\min(W_{lp})}.$$ To avoid division by zero, what I do (and this seems to preserve the measure's main purpose) is the following $$\\sum_{l=1}&#94;{k_1}\\sum_{p=1}&#94;{k_2}\\frac{\\max(W_{lp}+1)}{\\min(W_{lp}+1)}.$$ The addition \\(W_{lp}+1\\) implies that we add 1 to all elements in the window \\(W_{lp}\\) . I prefer to scale image to have intensities between 0 and 1, but it really is not important, you can use any range as long as it is float type. This is because of the nature of the division and \\(\\log\\) operators. And here's the code for this function: python def EME(image, window_width, window_height): height, width = image.shape sum_ = 0 k = 0 # I just decided not to keep # track of the blocks # window_height/ window_width variables take care of number of blocks H = np.int(np.floor(window_height / 2)) # range in height, distance from the # center of the window W = np.int(np.floor(window_width / 2)) # range in width, same as above for row in range(0 + H, height - H + 1, window_height): for column in range(0 + W, width - W + 1, window_width): window = image[row - H:row + H + 1, column - W:column + W + 1] I_max = window.max() I_min = window.min() D = (I_max + 1) / (I_min + 1) if D < 0.02: # this is also an underflow precaution D = 0.02 k += 1 sum_ += 20 * np.log(D) return sum_ / k Examples Let's try it out on some examples. What I am going to do here is, I will use the skimage library to obtain the data first. py from skimage.data import camera() # my favourite at this point My favourite sample image. And then lets use simple histogram equalization for image enhancement. This function will enhance the contrast of the image and will bring some details (but also noise! Don't 100% rely on it!) ```py from skimage.data import camera from skimage.restoration import denoise_bilateral, denoise_wavelet original = camera() enhanced = equalize_hist(original) image = rescale(enhanced, 0, 255).astype(\"uint8\") ``` And the result is pretty evident Quality comparison. Of course, histogram equalization produces artifacts, but visually the image is more detailed than originally which corresponds to increase in the measure, which is what we wanted from it in the first place! Question A question that I have for EME, so far just one, but I feel like it's important. How can we make it more resistant to noise? It does decrease for small amounts of noise: Adding Gaussian noise to the image (right) with $\\sigma=1$ decreases the quality measure. But once noise is pretty strong: Adding Gaussian noise to the image (right) with $\\sigma=10$ increases the quality measure. This is not good, particularly when the enhancement we perform somewhere implicitly brings large noise (such as histogram equalization!) forcing us to believe that the quality improved. The explanation is, of course, that due to random noise, we (with non-zero probability) increase the maximal and decrease the minimal values of pixels and thus cause the change in the final metric value. But how to make the metric more noise-robust?.. Well, there are certainly options. Some extensions listed here certainly could do the job and be noise-resistant. But this is probably for some future experiments, who knows!","tags":"Posts","url":"https://iliailmer.github.io/2018/12/image-quality/.html","loc":"https://iliailmer.github.io/2018/12/image-quality/.html"},{"title":"A surprising way sigmoid function is applied in computer vision","text":"Let's talk about all things image enhancement, what it is, why it is necessary and how do wavelets play a big part in it! Introduction Enhancement of images is an important preprocessing step in any image related system. Getting rid of noise, brightening, extraction of details - all of this helps in future steps, like feature extraction/engineering. There are many image enhancement techniques out there, let us look at one that uses a function which commonly is seen in neural networks Sigmoid function For all code below we will need matplotlib.pyplot, skimage and numpy . skimage is my preference of image processing library, I find it easy to understand and, if you want to modify something, you can always look under the hood of any function. So, we will need python from matplotlib import pyplot as plt %matplotlib inline for plotting, ```python from skimage.data import astronaut image = astronaut() from skimage.util import img_as_float64 # make image in range [0..255], 8-bit integers ``` for image, image datatype adjustment and for some feature visualization, respectively. Now, time for some visualization. python plt.figure(figsize=(10,9)) plt.imshow(image) plt.axis('off'); Our base image, astronaut. Let's make sure we have our sigmoid function correctly defined: python def sigmoid(x): return 1/(1+np.exp(-x)) A simple sigmoid. So, how do we use it to enhance the image? Well, let's look at the code: ```python image = astronaut() image_ = np.zeros_like(image, dtype=\"float64\") def sigmoid(x): return 1/(1+np.exp(-x)) def sigm_enh(I, alpha, beta): I = img_as_float64(I) I_out = I sigmoid(alpha (I-beta))# return I_out for i in range(3): image_[...,i] = sigm_enh(image[...,i], 2, 50)#float(image.mean())) fig, ax = plt.subplots(nrows=1, ncols = 2, figsize=(20,9)) ax[0].axis(\"off\") ax[0].imshow(astronaut()) ax[1].axis(\"off\") ax[1].imshow(rescale(image_,255)) ``` and the result: Application of the code above. Application of the code above with $\\alpha = -2,~\\beta=0.1$ and the result of rescaled to be between 0 and 255 for display. This is a simple way to bring image brightness up or down, especially comparing the results to some sort of metric (say, signal-to-noise ratio, or mean-squared error). In a different post, I will show a measure of image quality that our computer vision prof showed us (it's pretty cool, but has its own quirks). We should try to do a little better/advanced with image enhancement. A function introduced in this paper dubbed \"double sigmoid\" looks something like this \\( \\sigma_{double} = \\mathrm{sign}(x-x_1) \\exp \\left(1- \\frac{(x-x_1)&#94;2}{s}\\right)\\) And the plot of it python x = np.linspace(-10, 10, num=100) plt.plot(x, double_sigmoid(x,0,2)) which is similar to two sigmoids complementing each other. Using the technique from the paper on a colored image, we need to be careful because this method is very sensitive to input parameters. ```python image = astronaut() image_ = np.zeros_like(image, dtype=\"float64\") def double_sigmoid(x, x_1, s): return np.sign(x - x_1) * (1 - np.exp(-((x - x_1) / s) ** 2)) def double_sigm_enh(I, x_1, k, s, b): a = 1 / (double_sigmoid(k * (1 - b), x_1, s) - double_sigmoid(-k * (1 + b), x_1, s)) I_out = a * (double_sigmoid(k * (I - b), x_1, s) - double_sigmoid(-k * (I + b), x_1, s)) return I_out for i in range(3): image_[...,i] = double_sigm_enh(image[...,i].astype(\"float64\"), k=0.9, b=0.0, x_1=0, s=100) fig, ax = plt.subplots(nrows=1, ncols = 2, figsize=(20,9)) ax[0].axis(\"off\") ax[0].imshow(astronaut()) ax[1].axis(\"off\") ax[1].imshow(rescale(image_,255)) ``` This can also be applied to grey-level images. The problem with this method of enhancement is that it heavily relies on a careful choice of parameters and is very sensitive to their values. I really like that we can always find interesting applications of mathematical functions for image quality enhancement. In the next post I plan to talk about a measure that can be used to describe image quality.","tags":"Posts","url":"https://iliailmer.github.io/2018/11/image-enhancement/.html","loc":"https://iliailmer.github.io/2018/11/image-enhancement/.html"}]};